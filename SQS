SQS stand for Simple Que Service.

This AWS service help you de design a large scale system that work on que of a message.

There is two main part of the SQS
1. The Producer
2. The Comsumer

The producer are the software that generate the message and push it in the que. There can be many producer that keep pushing the messages in the que.

The consumer are the piece of software that consume that message and do the needful processing. Now there can be many consumer working at a time.
This consumer keep polling the que indefinatly for new messages. if the que has one then then the nexr poller consumer will get this message and start processing it.
Immediatly after that the message become invisible foe 'inVisibleTimout' amount of time.
Now its the responsibilty of consumer that he should delete this message from the que. if not then the message reapear in the que and re-processed.

Producer:-
  To send the message in SQS que use the in 'SQDTuto' folder

There is two way to write the consumer
1. In EC2 or on hosted server
2. Lambda

In EC2 or on hosted server :-
  This is kind of legacy and it need lot of work by developer himself. Dev has to write the code for indefinate polling and reading the message.
  
Lambda:-
  This is fatest and cheapest way to this. Here you do not have to write the polling mechanism at all.
  Actully when you set the SQS as event source in lambda then the Cloudwach also get involved in it.
  The cloudwach do the pollong of the que continously in the backround and when there is a new message/messages, the lambda get triggerd by the cloudwatch.
  
  This lambda can have more then one messages in one call which can be retrived from 'event.body.Records[]' array.
  if you wants to limit the number of record to read in on go then you can set the parameter  'batchSize' in lambda sqs event source. 



AccessPolicy of SQS : QuePolicy:-
  This is very much simillar to bucket policy. This can use to allow/deny the certain Principle for Certain operation on certain resources if certain condition is met.

SQS Pricing:-
  You get the 1st million of request for each month free. then it start charging.
  
  
Dead Letter Que?????????????

--------------------------
WHen exaclty we may need it :-
  WHen you need to make system that process hell lot of messages that lie one after another. 
  Suppose you need to process any upcoming image in S3 and process its metadat or create a meme or resize the image.
  and all this operations do not need to produce a response for the user.
  Its like user uploaded it and once uploading done user will goet the success response.
  But behind the picture after some time we will do that operation.
  
  How we can do it:-
    By lembda: yes you can. it has its advantage and disadvantage. thr disadvantage is if many people keep uploading then lot of lembda keep 
    invoking for this small operation.
    By SNS -
      ????????????
    By SQS:-
      This would be very suitable here. it goes like this
      1. First you need to create a message queue at  AWS > SQS > Create New > standard. 
      2. When S3 upload successful you deliver a message in that queue via this code
              [it may be uploaded by any melbda itself or you can confiure a lembga that will trigger once the upload is done 
                and this lembda will hvve below code]
          
          const params = {
            MessageBody: JSON.stringify({name: Mufazzal}),
            QueueUrl: "https://sqs.us-east-1.amazonaws.com/388412347424/mufQue"
          };
          sqs.sendMessage(params, (err, data) => { console.log(data, err); });          
      
        You can see this message in AWS console at 
          AWS > SQS > 'mufQue' > Action > view/delete message
       
       3. Now create Worker : You can use EC2-Auto scalling or Lembda to create a worker
          Worker will keep looking for the new messages and process it and delte it if you wants
            For EC2-Autoscalluing ??????????????
            For Lembda:  AWS > SQS > 'mufQue' > Action > configure Lembda
          USe this code         
          
              const params = {
                QueueUrl: "https://sqs.us-east-1.amazonaws.com/388412347424/mufQue",
                MaxNumberOfMessages: 1,
                VisibilityTimeout: 30,
                WaitTimeSeconds: 0
              };
              sqs.receiveMessage(params, (err, data) => {

                  const orderData = JSON.parse(data.Messages[0].Body);
                  const deleteParams = {
                    QueueUrl: queueUrl,
                    ReceiptHandle: data.Messages[0].ReceiptHandle
                  };
                  sqs.deleteMessage(deleteParams, (err, data) => {console.log(err, data;});
              }                                                                                                                                              });          
        
      Done.
      
      So the main usage of SQS is that when you have a stream of jobs coming for processing and you not need to process them in real time 
      you can do it later 
      for this you should create a SQS and deliver this jobs in that SQS in formate of a message which will have all the required info 
      for processing it.
      Now the worker thread will read it and process it.
      
      This is polling-based ,echanism. so the worker have to keep polling for the news message.
      It not a pullbased mechansn like SNS, the worker do not have any way to know that a new message if delivered in the queue.
      
      
      Type of Queue :-
        At the time of creation you can select the kind of que you wants to use for SQS. 2 choices
        1. Standard: This que guarantee that the message will be dilivere at least once but it may deliver the more than one copy
            of the same message and message may not pe processed in the same order as they are delivered.
            This is because the SQS system is highly distributed.
        2. FIFO: This que do some overhead work in background and make sure that messages are delivered onlty once and remain in the 
            exact order also. This is chargable abd can handle only up to 300 message/sec
      
      
      - max size of message is 256 KB
      - msg can kept in the que from 1 mnt to 14 days, default is 4 days. after this time it will be auto deleted.

-------

VisibilityTimeout: 
  When the worker start processing the message we need to make sure that at a time of processing no one accidently pick it again and start processing it in paraller.
  So we set this time to make this message incvisible for 30 sec so after 30 sec the message will re-apear in the que unless you have deleted it.
  So if you processing gona take more time then set this value accordingly.
  It can be configured :-
    - At queue level for the read by any consumer by setting in Queue param.
    - For specefic read operation by passing in 'VisibilityTimeout' param 'ReceiveMessage' api
        
------
  
Delay Que:-
  You can hide the message from the consumers of queue for certain amount of time after its delivered to queue.
  This gave you some time to do certain operations between delivering message to queue and processing it by consumer.
  It can be configured :-
    - At queue level.
    - For specefic message by setting 'message timer' for that message in 'SendMessage' API. 

------

Long Poling and Short Poling:-
  Whenever you poll the SQS que, you will get the unreaded message from the que.
  By default this return immedietly whichmns when you place the request, it check for new message if there is any they will be returned otherwise empty response will come.
  If there is fewer message in que the you will get the lot of empty response. This will bear higher cost on SQS as well as higher load on server. 
  To resolve this issue comes the long poling:-
    In the long polling instaed of returning result immedietly it will wait for few seconds for new messages. if there are new message then return it.
    max time it can wait is 20 sec. after that empty response will come.
    its default value is 0. and sp by default this will be a Short poll. 
    It can be configured :-
      - At queue level for the read by any consumer by setting in Queue param.
      - For specofic read operation by passing in 'WaitTimeSeconds' param 'ReceiveMessage' api

---------

Message:-
  It contains below element
    1. Message Body
    2. Message Attribute: up to 10 max 
    3. MessageId: This is generated when message is pushed in queue.
  For FIFO Queue there is additional Param:-
    1. Message deduplication ID
    2. Sequence number

---------

Purging Queue:
  If you don't want to delete an Amazon SQS queue but need to delete all of the messages from it, purge the queue. 
  The message deletion process takes up to 60 seconds. We recommend waiting for 60 seconds regardless of your queue's size.

--------------------------------

SQS and ASG:-
  Many of the time the SQS message are processed by the EC2. so its very logical to use ASG to launch and terminate the instance based on number messages in que.
  By this you do not have to keep the EC2 when there is no messages. or you can keep only the EC2 which are enogh to handle load.
  
  But we do not use numbe of mesage in the que, rather we us avrage BAcklog on EC2s in fleet.
  for this formula is ApproximateNumberOfMessages/running instances
  
  How:-
    1. Create custom CW metrics of backlog
    2. Make sure to publist this metric data in every 1 or 2 or n minute. 
    3. Create target tracking policy with acceptable value of backlog
    
---------

Issue of QueueName in cloudformation:-
   In CFL template if you specify the QueueName then you will not be able to perform any further update via CFL template on the que.
   So either do not give QueueName or give new name each time of update [so basically replace on each update] 

-------        
Size constraint of message:
  max allowed size is 256 KB
  For large message you can use 'Amazon SQS Extended Client Library for Java' which uses S3 to point the payload of the message 
  and could handle message of up to 2GB. Only JAva sdk work here. it only work though with AWS SDK. not with any ither.
    
  
  





    
