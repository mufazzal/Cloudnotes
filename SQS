SQS stand for Simple Que Service.

This AWS service help you de design a large scale system that work on que of a message.

There is two main part of the SQS
1. The Producer
2. The Comsumer

The producer are the software that generate the message and push it in the que. There can be many producer that keep pushing the messages in the que.

The consumer are the piece of software that consume that message and do the needful processing. Now there can be many consumer working at a time.
This consumer keep polling the que indefinatly for new messages. if the que has one then then the nexr poller consumer will get this message and start processing it.
Immediatly after that the message become invisible foe 'inVisibleTimout' amount of time.
Now its the responsibilty of consumer that he should delete this message from the que. if not then the message reapear in the que and re-processed.

Producer:-
  To send the message in SQS que use the in 'SQDTuto' folder

There is two way to write the consumer
1. In EC2 or on hosted server
2. Lambda

In EC2 or on hosted server :-
  This is kind of legacy and it need lot of work by developer himself. Dev has to write the code for indefinate polling and reading the message.
  
Lambda:-
  This is fatest and cheapest way to this. Here you do not have to write the polling mechanism at all.
  Actully when you set the SQS as event source in lambda then the Cloudwach also get involved in it.
  The cloudwach do the pollong of the que continously in the backround and when there is a new message/messages, the lambda get triggerd by the cloudwatch.
  
  This lambda can have more then one messages in one call which can be retrived from 'event.body.Records[]' array.
  if you wants to limit the number of record to read in on go then you can set the parameter  'batchSize' in lambda sqs event source. 


--------------------------

Mechanics:-
  Pushing message in Queue:-
      const params = {
        MessageBody: JSON.stringify({name: Mufazzal}),
        QueueUrl: "https://sqs.us-east-1.amazonaws.com/388412347424/mufQue"
      };
      sqs.sendMessage(params, (err, data) => { console.log(data, err); });          

  Now create Consumer and read messages: 
      You can use EC2-Auto scalling or Lembda to create a worker. USe this code         
          const params = {
            QueueUrl: "https://sqs.us-east-1.amazonaws.com/388412347424/mufQue",
            MaxNumberOfMessages: 1,
            VisibilityTimeout: 30,
            WaitTimeSeconds: 0
          };
          sqs.receiveMessage(params, (err, data) => {

              const orderData = JSON.parse(data.Messages[0].Body);
              const deleteParams = { QueueUrl: queueUrl, ReceiptHandle: data.Messages[0].ReceiptHandle};
              sqs.deleteMessage(deleteParams, (err, data) => {console.log(err, data;});
          }

  - This is polling-based ,echanism. so the worker have to keep polling for the news message.
  - It not a pullbased mechansn like SNS, the worker do not have any way to know that a new message if delivered in the queue.
  - You must delete the message otherwise it will re-appear once visibilityTimeOut elapsed.

-------

Type of Queue :-
  At the time of creation you can select the kind of que you wants to use for SQS. 2 choices
  1. Standard: 
      This que guarantee that the message will be dilivere at least once but it may deliver the more than one copy
      of the same message and message may not pe processed in the same order as they are delivered.
      This is because the SQS system is highly distributed.
  2. FIFO: 
      This que do some overhead work in background and make sure that messages are delivered onlty once and remain in the 
      exact order also. This is chargable.
      It name must end with .fifo sufix.

-------

Limits:-
  MAx size of message: 256KB.
  Throuput limit of Standard SQS: unlimited.
  Throuput limit of FIFO SQS: It is finite. 
    You can have max 300 transaction per sec per api(SendMessage, ReceiveMessage, or DeleteMessage) without batch. OR
    You can have max 3000 transaction per sec per api(SendMessageatch, ReceiveMessage, or DeleteMessageBatch) with batch
    ** 3000 = 300 * MAximun message possible in Batch = 300 * 10

-------

VisibilityTimeout: 
  When the worker start processing the message we need to make sure that at a time of processing no one accidently pick it again and start processing it in paraller.
  So we set this time to make this message incvisible for 30 sec so after 30 sec the message will re-apear in the que unless you have deleted it.
  So if you processing gona take more time then set this value accordingly.
  It can be configured :-
    - At queue level for the read by any consumer by setting in Queue param.
    - For specefic read operation by passing in 'VisibilityTimeout' param 'ReceiveMessage' api
    - MAx possible value = 12 hour
        
------
  
Delay Que:-
  You can hide the message from the consumers of queue for certain amount of time after its delivered to queue.
  This gave you some time to do certain operations between delivering message to queue and processing it by consumer.
  It can be configured :-
    - At queue level.
    - For specefic message by setting 'message timer' for that message in 'SendMessage' API. 

------

Long Poling and Short Poling:-
  Whenever you poll the SQS que, you will get the unreaded message from the que.
  By default this return immedietly whichmns when you place the request, it check for new message if there is any they will be returned otherwise empty response will come.
  If there is fewer message in que the you will get the lot of empty response. This will bear higher cost on SQS as well as higher load on server. 
  To resolve this issue comes the long poling:-
    In the long polling instaed of returning result immedietly it will wait for few seconds for new messages. if there are new message then return it.
    max time it can wait is 20 sec. after that empty response will come.
    its default value is 0. and sp by default this will be a Short poll. 
    It can be configured :-
      - At queue level for the read by any consumer by setting in Queue param.
      - For specofic read operation by passing in 'WaitTimeSeconds' param 'ReceiveMessage' api

---------

Max Batch size: 
  define how many maximum message can be read in single read operation by any consumer.
  It can be configured :-
    - At queue level for the read by any consumer by setting in Queue param.
    - For specefic read operation by passing in 'MaxNumberOfMessages' param 'ReceiveMessage' api
        


----------

Message:-
  It contains below element
    1. Message Body
    2. Message Attribute: up to 10 max 
    3. MessageId: This is generated when message is pushed in queue.
  For FIFO Queue there is additional Param:-
    1. Message deduplication ID
    2. Sequence number
    3. Message Group Id:- Use this if you wants to specifies that a message belongs to a specific message group.
          Message in same group will always be processed in order and by same consumer.
          however, messages that belong to different message groups might be processed out of order
          By default if there is not group id then message will be procesed in absolute order. 
    
  - Max size of message = 256KB 
  
    Message Attribute:-
      Message attributes are optional and separate from—but are sent together with—the message body. 
      The receiver can use this information to decide how to handle the message without having to process the message body first.
      Each message attribute consists of Name, Type, VAlue. 
  
    For Message > 256KB
      For large message you can use 'Amazon SQS Extended Client Library for Java' which uses S3 to point the payload of the message 
      and could handle message of up to 2GB. Only JAva sdk work here. it only work though with AWS SDK. not with any ither.


---------

Purging Queue:
  If you don't want to delete an Amazon SQS queue but need to delete all of the messages from it, purge the queue. 
  The message deletion process takes up to 60 seconds. We recommend waiting for 60 seconds regardless of your queue's size.

--------------------------------

SQS and ASG:-
  Many of the time the SQS message are processed by the EC2. so its very logical to use ASG to launch and terminate the instance based on number messages in que.
  By this you do not have to keep the EC2 when there is no messages. or you can keep only the EC2 which are enogh to handle load.
  
  But we do not use numbe of mesage in the que, rather we us avrage BAcklog on EC2s in fleet.
  for this formula is ApproximateNumberOfMessages/running instances
  
  How:-
    1. Create custom CW metrics of backlog
    2. Make sure to publist this metric data in every 1 or 2 or n minute. 
    3. Create target tracking policy with acceptable value of backlog
    
---------

Issue of QueueName in cloudformation:-
   In CFL template if you specify the QueueName then you will not be able to perform any further update via CFL template on the que.
   So either do not give QueueName or give new name each time of update [so basically replace on each update] 

-------        

Size and Time constraint of message:
  - max size of message is 256 KB.  
  - msg can kept in the queue from 1 mnt to 14 days, default is 4 days. after this time it will be auto deleted.

  For Message > 256KB
    For large message you can use 'Amazon SQS Extended Client Library for Java' which uses S3 to point the payload of the message 
    and could handle message of up to 2GB. Only JAva sdk work here. it only work though with AWS SDK. not with any ither.

--------    

QuePolicy:-
  This is very much simillar to bucket policy. This can use to allow/deny the certain Principle for Certain operation on certain resources if certain condition is met.

--------

SQS Pricing:-
  You get the 1st million of request for each month free. then it start charging.
  
----------

SQS Error HAndling:-
  When Consumer start processing the message it may success and delete the message of fail and not able to delete the message in the end.
  In this case message may appear in the queue after VisibilityTimeOut. 
  Now in case of Standard Queue, SQS will retry this message later and keep trying it till message reetention priode passed out or message succesd.
  And in case of FIFO queue, that message group remain unavaialble and keep retrying. 
  This retries of failed message can shoot the hardware usage and overhead. To resolve this comes

  Redrive policy and Dead Letter Que:-
    DLQ is a another queue which has the message which are failed. To configure it you must set up the redrive policy.
    in redrive policy you set
     - DLQ url 
     - max fail count after which message will move to DLQ.
    
-----------

Batch Action:-
  There are three operation you can do on the batch of the messages. 
  1. SendMessageBatch
  2. DeleteMessageBatch
  3. ChangeMessageVisibilityBatch
  - This will reduce the cost and load as you will need less API call for those operation.
  - As you always read message in batches, its is good idea to delete then in single API call rather then deleting each message indivisiually.
  - A batch can contain max 10 messages.
  
---------------

SQS Encryption:-
  Encryption at rest:-
    You can enable SQS to encrypt the message in the queue. This message will be encrypted as soon as it arrive and then stored in hardware.
    Now it will be decrypted and send when ant authorized consumer read it. yiu can use CMK for encryption.
    - SSE encrypts the body of a message not attriobute and metadata
    - When Amazon SQS moves a message from an encrypted source queue to an unencrypted dead-letter queue, the message remains encrypted
  Encryption at transit:-
    For this use TLS
  
  
----------------------



Amazon MQ:-
  MQ is used if you are using any other message API or message broker service apart from SQS.
  You can migrate that messaging service to cloud via Amazon MQ. 
  MQ simplty provide a way to integrate the third party service that are in direct competision of SQS, into the AWS cloud 

  
