PRovision Throughput CApicity:-
   This is used to configure hom much dat can be write or read per second. This value should be chosse on the basis of
   how much user uses the DDB table at a given time and what is the pattern of usage.
   You can set it at the time of table creation or later when created already.
   For new table , its at first form only
   for exisying table  AWS > DDB > table > 'your table' > capacity.
   Some fundamental:- 
     In DDB you generally write data in the cell and the data containg of couple of Attribute with keys.
     Now first thing you have to figure out is to answer this 4 question
       A. How much the size of your each data item that you are going to read and write.
       B. How many read operation you need to do in a second
       C. How many wtrite operation you do in a second
       D. You need Eventually-consistent read or Strongly-consistent read 
     This is solely depend on your app load.

   There is two way to set them
   1. Pre assigned (with auto scalling if you wants)		   
         Now when you were create the table in AWS console, you can see a calculator to calculate the provision there.
         fill all four value there and you will see 3 numbers in output
          1. The read capacity unit you need 
          2. The write capacity unit you need
          3. The cost you are going to bear for this.
            - So if size your data is large you have to provision higher capascity and have to bear higher cost.
            - If you have lot of data read operation then give higher capacity to read and lesser to write and vise-versa
            - Strongly consistent read will bear the higher cost.
        Now what will happen if suddenly load increase for any read or write operation, For this you set the 
        Auto-Scalling for read:-
          Simply set if to allot minimum   <MNMNMN> unit 
          for read capacity anf if utilization goes above   <PPPP> percentage 
          then add  one more unit and keep it going till the allotment reach to       <MXMXMX> unit
          After that let the thing crash or performance dipped.

        Auto-Scalling for write:-
          Simply set if to allot minimum   <MNMNMN> unit 
          for write capacity anf if utilization goes above   <PPPP> percentage  
          then add  one more unit and keep it going till the allotment reach to       <MXMXMX> unit
          After that let the thing crash or performance dipped.

			Calculation legend:-
				- Always round up the data size to nearest factor of 4
				- 1 RCU = 2 * 4KB  for Eventual consistent
					1 RCU = 1 * 4KB  for Strong consistent
					1 WCU = 1 * 1KB  


   2. On demand
      This is new and the best one. just enbale it and you do not need to set anything . AWS will take care of all things.


  PRovisio VS On-demand:
    PRovision is best if you know the traffic pattern and load on the DB because you can  proviosn capacity and pay less.
    On demand if you do not know the traffic pattern and load on the DB and also your app is new in market so you belive low traffic.
       		
	
	PRovision CApicity and GSI :-
		When you creat a GSI, it as same as creating a new table. so at that time you will also be asked to set the provision-capasity of 
		GSI table. all the concept same here also except GSI table do not support 'on demand' provisioning.
		So more GSI will cost you more.

	
	What if your throughput goes exceptionalyy high and surpass even the autoscalling max:-
	In that case you will get the 'ProvisionedThroughputExceededException'
	Now if tou are using any AWS-SDK then SDK (if js, java or any) will take care of this and do the retries via 'Exponential Backoff Algorithm'
	SDK handle all this internally so no worry for you.
	But even if retry fail for 1 minute then its a dead end. you will get the error and you have to increase the provisioned capacity.
   
   
--------------------------

DDB Accelerator [DAX] :-
	DAX in in-memory cache for DDB. 
	DAX provide you way to let the frenetly accessed data to remain in cache.
	For this you 
		set up the DAX, 					[HOW ????????????????????????????]
		Point all your API to DAX instead of DDB table		[HOW ????????????????????????????]
	then AWS DDS system take care
	if your API reach to DAX and try to get some data but DAX do not find it then
	DAX make a getItem call to DDB table get it
	Store it in DAX cache
	and return it to api response
	Next time if some api ask for same data then it will just return from the pre-stored cache.
        default cache TTL is 5 mnt but you can change it
  Advantages :-
		- remove many read overload from DDB and so you may need less read-capacity on DDB table
		- Make read fater
		- Cache the 'HotKeys' for you so that ferquetly asked data are deliver fast and consume no read capacity at DDB
		- It give you 10X performance and provide Single digit Microsecond latency (10 time better then milisecond latency of DDB withot DAX)
   Limits:-
	Not major improvement if data is changing frequently
		If data change in origin DDB table it still may remain unupdated in cache   
		so its only suitable for eventually consistent read. not suitable for Strongly-consistent read.
		
---
	DAX and eventually consistent reads:-
		For Any eventually consistent reads by GetItem, BatchGetItem, Query and Scan the data is forst read in cache if its there then return it
		othrwise go to DB and find it, Cache it and deliver to end client.
	DAX and strongly consistent reads:-
		This is not supported by DAX, the request is forwarded to DDB and response from DDB is forwarded to client whithout even csching the result.
	DAX and write operation:-
		DAX supporrt two kind of write operation for BatchWriteItem, UpdateItem, PutItem, DeleteItem
		1. Write-Through:-
				The DAX forward the request to the DDB first once DDB is updated then the DAX cache also updates. and then operation completed.
				So Write via DAX will kepp DDB and DAX both in Synch in real time. This is default behaviour.
		2. Write-Aroud:-
				This is useful if you wants to write a bulk data which is not read frequently. Here data is written to DDB directly.
				The DAX will load it only when client request for it.
--
	LRU and TTL:-
		By default TTL of each item in DAX is 5 mnt but you can change it. DAX also main tain the 'least recently used' for each cached item.
		In case the Cache becoming full, the DAX will delete the item which has not been read in longest time even if there TTL has not passed.		
--
	Cache:-
		DAX maintain two cache, 
		one for GetItem and BatchGetItem called 'Item Cache' where result are stored by their primary key values and 
		other for Query and Scan called 'Query Cache' where result are stored by their parameter values
--
	DAX Infra:-
		DAX cluster is launched in the VPC and you need to select in which AZ youz DAX nodes will lies.
		You can also select the SG to allow specifit traffic on your nodes. THe DAX cluser can subordinate to the DDB in same region only.
		The DAX cluster has one 'Cluster Endpont' and for each node a one 'Node Endpoint'

----------------------

ElastiCache with DynamoDB:-
	You can use ElastiCache too with DDB. It is also In-Memory DB like DAX 

----------------------
Hot Parttion Key Issue:-

	When an Amazon DynamoDB table is created, you can specify the desired throughput in Reads per second and Writes per second. 
	The table will then be provisioned across multiple servers (partitions) sufficient to provide the requested throughput.

	You do not have visibility into the number of partitions created -- it is fully managed by DynamoDB. 
	Additional partitions will be created as the quantity of data increases or when the provisioned throughput is increased.

	Let's say you have requested 1000 Reads per second and the data has been internally partitioned across 10 servers (10 partitions). 
	Each partition will provide 100 Reads per second. If all Read requests are for the same partition key, the throughput will be limited to 100 Reads per second. 
	If the requests are spread over a range of different values, the throughput can be the full 1000 Reads per second.

--------------------

Getting data:-
	- For retriving multiple partition key u can use BatchGetItem as single operation. 
	- Try to avoid scan item as much as possible. if you still use it then use --page-size to break it in small operation.
	- A single scan on large data table can eat all the proviosion throuput and throttle other requests.
	- Use Parallel scan for fast result.

--------------

Limits:-
	Max GSI: 20 [Above it you will need to raise the support ticket]
	Max Provison capacity: ???? [Above it you will need to raise the support ticket]




   
