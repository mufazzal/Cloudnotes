PRovision Throughput CApicity:-
   This is used to configure hom much dat can be write or read per second. This value should be chosse on the basis of
   how much user uses the DDB table at a given time and what is the pattern of usage.
   You can set it at the time of table creation or later when created already.
   For new table , its at first form only
   for exisying table  AWS > DDB > table > 'your table' > capacity.
   Some fundamental:- 
     In DDB you generally write data in the cell and the data containg of couple of Attribute with keys.
     Now first thing you have to figure out is to answer this 4 question
       A. How much the size of your each data item that you are going to read and write.
       B. How many read operation you need to do in a second
       C. How many wtrite operation you do in a second
       D. You need Eventually-consistent read or Strongly-consistent read 
     This is solely depend on your app load.

   There is two way to set them
   1. Pre assigned (with auto scalling if you wants)		   
         Now when you were create the table in AWS console, you can see a calculator to calculate the provision there.
         fill all four value there and you will see 3 numbers in output
          1. The read capacity unit you need 
          2. The write capacity unit you need
          3. The cost you are going to bear for this.
            - So if size your data is large you have to provision higher capascity and have to bear higher cost.
            - If you have lot of data read operation then give higher capacity to read and lesser to write and vise-versa
            - Strongly consistent read will bear the higher cost.
        Now what will happen if suddenly load increase for any read or write operation, For this you set the 
        Auto-Scalling for read:-
          Simply set if to allot minimum   <MNMNMN> unit 
          for read capacity anf if utilization goes above   <PPPP> percentage 
          then add  one more unit and keep it going till the allotment reach to       <MXMXMX> unit
          After that let the thing crash or performance dipped.

        Auto-Scalling for write:-
          Simply set if to allot minimum   <MNMNMN> unit 
          for write capacity anf if utilization goes above   <PPPP> percentage  
          then add  one more unit and keep it going till the allotment reach to       <MXMXMX> unit
          After that let the thing crash or performance dipped.

   2. On demand
      This is new and the best one. just enbale it and you do not need to set anything . AWS will take care of all things.


  PRovision	VS On demand:
    PRovision is best if you know the traffic pattern and load on the DB because you can  proviosn capacity and pay less.
    On demand if you do not know the traffic pattern and load on the DB and also your app is new in market so you belive low traffic.
       		
	
	PRovision CApicity and GSI :-
		When you creat a GSI, it as same as creating a new table. so at that time you will also be asked to set the provision-capasity of 
		GSI table. all the concept same here also except GSI table do not support 'on demand' provisioning.
		So more GSI will cost you more.

	
	What if your throughput goes exceptionalyy high and surpass even the autoscalling max:-
	In that case you will get the 'ProvisionedThroughputExceededException'
	Now if tou are using any AWS-SDK then SDK (if js, java or any) will take care of this and do the retries via 'Exponential Backoff Algorithm'
	SDK handle all this internally so no worry for you.
	But even if retry fail for 1 minute then its a dead end. you will get the error and you have to increase the provisioned capacity.
   
   
--------------------------

DDB Accelerator [DAX] :-
	DAX in in-memory cache for DDB. 
	DAX provide you way to let the frenetly accessed data to remain in cache.
	For this you 
		set up the DAX, 					[HOW ????????????????????????????]
		Point all your API to DAX instead of DDB table		[HOW ????????????????????????????]
	then AWS DDS system take care
	if your API reach to DAX and try to get some data but DAX do not find it then
	DAX make a getItem call to DDB table get it
	Store it in DAX cache
	and return it to api response
	Next time if some api ask for same data then it will just return from the pre-stored cache.
        default cache TTL is 5 mnt but you can change it
  Advantages :-
		- remove many read overload from DDB and so you may need less read-capacity on DDB table
		- Make read fater
		- Cache the 'HotKeys' for you so that ferquetly asked data are deliver fast and consume no read capacity at DDB
   Limits:-
	Not major improvement if data is changing frequently
		If data change in origin DDB table it still may remain unupdated in cache   
		so its only suitable for eventually consistent read. not suitable for Strongly-consistent read
         
----------------------


Hot Parttion Key Issue:-

	When an Amazon DynamoDB table is created, you can specify the desired throughput in Reads per second and Writes per second. 
	The table will then be provisioned across multiple servers (partitions) sufficient to provide the requested throughput.

	You do not have visibility into the number of partitions created -- it is fully managed by DynamoDB. 
	Additional partitions will be created as the quantity of data increases or when the provisioned throughput is increased.

	Let's say you have requested 1000 Reads per second and the data has been internally partitioned across 10 servers (10 partitions). 
	Each partition will provide 100 Reads per second. If all Read requests are for the same partition key, the throughput will be limited to 100 Reads per second. 
	If the requests are spread over a range of different values, the throughput can be the full 1000 Reads per second.











   
