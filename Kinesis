Amazon Kinesis is a massively scalable and durable rad:-real-time data streaming service
It can capture gigabytes of data per second from hundreds of thousands of sources such as
  website clickstreams, 
  financial transactions, 
  social media feeds, 
  IT logs, 
  location-tracking events
  
  Kinesis Data stream:-
    KDS collect the data from diffret stream of sources and place in Shrads. This data can remain in the stream for 24 hour(by default) 
      But You can raise this limit to up to 7 days by enabling extended data retention or up to 365 days by enabling long-term data retention
    you can create a consumer application to read and intrpret this data from stream.
    Generally you can use this stream of data to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing.
    KDS has its own data storage.
    
    Producer:- 
      A piece of software who place the dat in the stream's shrads by callint 'PutRecord' api. You can use Kenesis Producer Lib of rthis
    
    Consumer:-
      an application that you build to read and process data records from Kinesis data streams.
      The consumer could be 
      - Lambda with Evenr Source mapping with the stream
      - You can use an Amazon Kinesis Data Analytics application to process and analyze data in a Kinesis stream using SQL, Java, or Scala
      - You can use Kenesis stream also if you wants to futther transform and then store the result in any aws manged srvice.
    
    Shrad:-
      A shard is a uniquely identified sequence of data records in a stream. A stream is composed of one or more shards, each of which provides a fixed unit of capacity
      Each shrad can have capacity of
        5 transaction per sec for read [limited to 2MB per sec]
        1000 transaction per sec for write [limited to 1MB per sec]
        The total capacity of the stream is the sum of the capacities of its shards. To increase the capacity of Stream you can increase the number of shrads.
      - GEnerally the consumer of KDS is EC2, EC2-Fleet, Lambda, EMR which can have some kind of processing capacity.
      - You set the initial number of total shard in the stream at the rime of creation. You can later change it if you wants.
      - Number of shrad define the throuput of the Kenesis stream higher the number of shrad increase the througput.
      - In downstream you can have multiple consumer like one real-time dashboard and another for saving data in any storage.
      
      Shard Partitin Key:-
        Each shard must be assosiated with a unique partition key. When producer place the data record in the stream, it must also present the partition key.
        The stram will place the data record in the shrad shrad who has matching partition key value.
        In your application logoc you shoudl choose shrad as that it have lot more possible values then number of shrad running at a time.
        This ways data will be spread across the shrad properly.
      
      ReShrading: Depending on the load on your system you may wants to change the number shard in the stream. this is called resradding. you can do it by
      1. Chnaging value by caling api UpdateShardCount API.
      2. By spliting the shrad to increase the number of shrad
      3. By merging them to decrease the number of shrad
      
      Scalling:-
        You have to set the value of max number of shard in stram which defime the maximum scalling of the system throughput.
        
      Performance:-
        In case yur producer is producing lot of message and consusmer is lagging to process it then you can configure the consumer to read the message in Batches
        so that in one go you can process more messages.
----------------        
  Kinesis Data Firehose:-
    This too collect the data from multiple stream but instead of storing them in Shrads it deliver the data to any AWS storrage service like 
      > S3,
      > Redshift, 
      > Amazon Elastic search
      > Splunk (non aws)
      > HTTP endpoint (non aws)
      > MongiDB
    KDF do not have any of its own storage or shrads. it just deliver the data to storage service.
    The KDF can source the data from one of
      > KDS
      > Cloudwarch Log and event
      > KPL
      > Kenesis Agent
      
    Later you can use feature of that service to analyze the data.
    
    Transform source records with AWS Lambda:-
      If you do not wnats to store the data in same way as it arive and wnats to make some changes like Compressing, Fitering, Transforming
      Then you can enable this and use the Lambda function to perform this operation on data item before saving in destination.
      IMP: This is diffrent from KDS+Lambda as KDS+Lambda is event source mapping where lambda keep polling the KDS for data and retrive many data cell in one attempt.
            IN KDF+Lambda, the lambda is called by KDF for each data cell of KDF. 
            Also here the uotput of lambda must be propery formated so that it can be saved in downsteam storage system.
    Convert record format:-
      Use it for converting the formate of data.
    
    Scalling:-
        Unlike KDS, in KDF You do not have to set the value of max number of shard in stream. So it can scal without any manual provisioning.
----------------  
  Kinesis Data Analytics:-
    Amazon Kinesis Data Analytics is the easiest way to transform and analyze streaming data in real time with Apache Flink or woth SQL like queries
    KDA work in conjection with KDS or KDF. Ypu cannot ingest data in the KDA directly.
    Data ingested in the KDS/KDF first then you can integrate the KDA to analyze data directly in real time.
    Its fully managed so no need to set up a server or auto scalling
    Thers is two way:-
      - By Apache Flink
      - By SQL like queries.
--------------

Encryption:-
  - In Kenesis Stream you can choose to have encryption at rest enable or disabled. IF it is enable then the data is 
      encrypted as it enter in the stream by any producer
      and decrypted right before delivering to any consumer.
  - The producer and Consumer has not to worry about any crypto operation.
  - it encrypts incoming data only after encryption is enabled. Preexisting data in an unencrypted stream is not encrypted after server-side encryption is enabled
  - You can use Cutomer or AWS managed CMK.

--------------

KDS:  Ingest from source -> Put in shrads -> Process via EC2/Lambda/EMR.
KDF:  Ingest from source -> Trasform/Filter by lamba if needed -> Store in analytics storage like S3/Reshift/ElsticSearch/MongoDB 
KDA:  Ingest from KDF/KDS -> Run query/Apache flink -> analyze in dasborad or put reult in any destination
      
--------------

Kenesis use case:-
  KDS:-
    Suppose you have a bus company and have an app to trach them all. All your busses send GPS signal each minute.
    You can create real time tracking and monitoring by kenesis
    1. Create Kensis stream with number of shrad = average number of busses to track at a time.
    2. Use one shrad for each indivisual bus. use busId as partition key for shrad.
    3. Creat client app and install it in all the buses which will push GPS cordinate in the shrad with appropiate partition key.
    4. Now the busses will push and populate the data in shrad. Data in each shrad are in proper sequence wrt timline of GPS cordinate of bus.
    5. Now in downstream you can process data via lambda/EC2/EMR and produce some result. 
  KDF:-
    Here you do not need shrad because you just need to store the data (with transformation if needed).
    1. Create the KDF 
    2. Enbale 'transformation by lambda' and 'formate conversion' if needed and so select lambda if it is enabled.
    3. Store data for analytics in S3, Redshift, EMR etc.
  
------------------  

  SQS vs KDS:-
    KDS have many que in form of shrad.
    SQS has single que.
    
    KDS guarantee order in the shrad (not among the shrad).
    SQS do not guarantee order but shrad 
    
    In KDS you can have multiple downstraem system for processing the data item. 
    In SQS message can only be processed by one consumer. in 
    
    KDS is limit by number of shrad provisioned in it. so its scallibity with number of shrad is limited. Though messages in the shrad is limitless.
    SQS can scale indfinetly with number of messages in que and no need to provision capacity.
    
    KDS is best for REAL-TIME-HIGH-VOLUE data streaming processing upto giga to prtabyte per sec. It deliver the message to the consumer in around 70 milisecond.
    SQS is best for DECOUPLING of an applications.
        
  ----------------
  
  Consumer:-
    For creating Consumer either you can use LAmbda or use Kenesis-Consumer-lib(KCL) in any EC2 or on-premise server.
    If you are using KCL then you should launch the number of instance in factor of number of shrad
    EG:-
    If you have 2 shrad then you should have 1 or 2 instance.
    If you have 5 shrad then you should have 1 or 5 instance.
    If you have 6 shrad then you should have 1 or 2 or 3 instance.
    If you have 10 shrad then you should have 1 or 2 or 5 or instance.
    
    Reason is that a shrad is handled by a single KCL all the time. though a single KCL can take multiple shrad.
    Also max number of Instacne you should have = number of shrad because extra instance will have nothing to serve.  
    
    Multiple Consumer:-
      You can have multiple consumer to consume the shrad in parallel.
      For this you can use the Fan-Out feature of kenesis.
      Without fan out, all  of the consumer will have 2MB/sec/shrad throuput
      With    fan out, each of the consumer will have 2MB/sec/shrad throuput
      
    
  ---------
  Calculating Initial Number of shrad:-
    Incoming write Bandwith = data size * Record per sec
    Outgoing read Bandwith = Incoming write Bandwith * No of Consumer
    Number of shrad = Max (Incoming Bandwith, Outgoing Bandwith)
    
------------
Useful Library and Tool:
  Kenesis PRoducer lib: Use this in the aplication to send data  to the kenesis.
  Kenesis Agent: Use this to send data the Kinesis in the background. It moniter the set of file and send the new data in the file in reliable manner.
  Kenesis Consumer lib: use this to contnously read the kenesis shard and process it in the application 


        
    
