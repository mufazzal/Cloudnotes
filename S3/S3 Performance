

There are diffrent technique you can adopt for performance optimization for diffrent kind of operation.

Technique                 Target                                    Operation            How               

Transfer Acceleration     Latency minimization                      GET and PUT          Instead of simple internet, it uses Amazon backbon network                                                       
CloudFront + S3           Latency minimization                      GET                  By caching the content in nearest edge location. and us Amazon backbon network to get object if no cache there.
Multipart Upload          Throughput / maximise bandwith usage      PUT                  By breaking large file in small parts and upload them in paraller. remerge them in the end of operation.
Range Based Get           Throughput / maximise bandwith usage      GET                  By breaking large file in small parts and download them in paraller. remerge them in the end of operation.      
Parallelise
Secondery Indexes         Faster response by s3                     LIST                  ??????
S3 Inventory              faster response by s3                     LIST                  ??????            
            
       
Default performance of S3:-
  3500 PUT/POST/COPY/DELET per sec per prifix
  5500 GET/HEAD per sec per prifix
      

Transfer Acceleration:-
  The TA uses the AWS owned global clodfront network(instead of internt) to transfer the data.
  Data arrive at edge location first and then passed to destination address over the AWS backbone network.
  Data is not stored/cached at Edge location at all. it just use the backbone network.
  For GET:-
    1. Request pssed to nearest edge location 
    2. the edge location then use the backbone network(not internet) to get the object, 
    3. Object routed to caller address via internet netwprk.
   For PUT:-
    1. Request pssed to nearest edge location and object uploaded to the edge location. 
    2. the edge location then use the backbone network(not internet) to upload the object to S3 bucket, 
    3. Success/Fail response routed to caller address via internet netwprk. 
  
  Enabling TAX by CFR:-
    simply add property
        AccelerateConfiguration:
          AccelerationStatus: Enabled    
   
   Now you can use the TAX url of this buckey for GET and PUT operation
   TAX url formate-
    https://<Bucket Name>.s3-accelerate.amazonaws.com/<Key>
    
  Doing upload/GET via JS SDK with TAX-
      var s3 = new AWS.S3({apiVersion: '2006-03-01', useAccelerateEndpoint: true});
      const params = {Bucket: ..,  Key: .., ..}
      s3.upload(params).promise().then(res => console.log(res)).catch(rej => console.log(rej));
      OR
      s3.getObject(params).promise().then(res => console.log(res)).catch(rej => console.log(rej));
    Fir CLI-
     aws configure set default.s3.use_accelerate_endpoint true
    
 TAX url and Access:-
   In the SDK and CLI you just need to set the useAccelerateEndpoint=true to use this feature. SDK internally take care of other things then.
 
 
 CloudFront + S3:-
    What if your 
      - user base is spread accross the world and 
      - most of the user do GET operation and
      - your bucket is in one location.
    due to this Even TAX may not help to all user because S3 bucket will have lot of GET request to handle from lot many user.
    For this comes the CDN(Cloudfront for AWS)
    The CFT use the globally spreaded EdgeLocations to cache the bucket content and deliver the object from nearest EdgeLocation instead of bucket.    

    More on CFT:- see 'S3: Performance: CloudFront.txt'
       
        
        
  MultiPart Upload:-
    For a very lagrge object The AWS can split it into small piece and upload thenm in parallel. 
    The SDK be default do it. If size > 10MB then SD/CLI splitthen in pieces of 10MB and upload them all in parallel.
    so for 100MB file there will be 10 request of 10MB each.
    
    If you wants you can change the default chuck size from 10MB to other value.
    How ?????????
    There is some limit here
      max size: 5TB
      max possible parts: 10000
      part size: 5MB to 5GB
      
 S3 Inventory:-
   You can set the inventory to get quick snapshot of detail of objects in the S3 bucket. 
   This will give you info about all the object or of some special prefix in CSV formate. 
   You can set to generate weekly or daily.
   

S3 Select:
  S3 select is design to query deep inside the object with SQL alike query. With this you do not have to load the whole object instead a littile imp data from Object you can load.
  Currently it support onlu CSV, JSON and ZIP formated objects.
  
  Suppose you have a object user.csv in S3 then you ca use S3 select like-
  
    const query = ;
    const bucket = 'test-bucket-for-s3-select-tutorial';
    const key = 'planets.json';
    const params = {
      Bucket: user,
      Key: 'userPosts.json',
      ExpressionType: 'SQL',
      Expression: 'SELECT * FROM s3object[*].results[*] r;',
      InputSerialization:  { JSON: { Type: 'DOCUMENT',}},
      OutputSerialization: { JSON: { RecordDelimiter: ',' }}
    }  
  
  
  
  
      
    
    
    
     
    
  
  
