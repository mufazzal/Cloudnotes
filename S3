In very Simple word S3 (Simple Storage Service) is STATIC STORAGE. 
A bucket that is best place to put static HTML, css, video, image, static JS  etc.

Things that should be stored in S3 :-
  Any kind of flat file like image, .html. video etc
Wich should not:-
  Any dynamic/processable/database/OS file
  
 
Bucket:-
  In very simple word, This is a kind of UNIQUE folder in the ENTIRE S3 UNIVERSE
  So when you create it, its name must be quique (not relative to the buckets in your S3 account but in an entire S3 unviverse)
  So you can not take name like test, test123 etc as they may have taken centuries ago. 

Region of S3:-
  S3 is region independent, when you open S3 in aws console you will see region selected as 'Global' and other rigion remian unselectable.
  It means that S3 service remain same in entire aws system.
  If "Test" bucket in created in region "Asia" then "Test" cant be created again by anyone in any region ever.
Region of bucket:
  Though S3 is region independent, but the bucket created in S3 by you will must lie in a selected region 
  
Q:  Can i access the bucket on region A in Region B
  ????????
 

How To create Bucket :-
  1. aws > S3 > Create Bucket
  2. Fill Name: Mut be unique in all S3 universe 
  3. Select Region: You should select nearest one (VVIMP: Though you select region here but still the name should ne unique to entire S3 universe )
  4. Couple of more thing (will be covered later)


S3 and Data Consistency :-
1: Read after Write: its consistent, mns you will be able to read the file immdietly after writing it.
2: Read after Delete/Change: This may take some time to reflect, so if you read immedietly after then you may get the previouse version



S3 and Region :-
  When you create a bucket youu have to select the region, and so toyr bucket will lie in that region. 
  VVIMP: it not at all means that content of this bucket wont be avialabler in other region.
  like you create a bucket 'indiaMapImages' in mumbai. 
  but the url 'https:// indiaMapImages.s3.eu-west1.amazonaws.com/pune' will be accessible in anywhere in the world.
  Though the time taken to load the content in newyork will be greater than in mumbai.


S3 and AZ :- 
  Bucket in region specific, you do not have to choose the AZ at all when you create bucket.
  Depending on your storage class, the content of the bucket will be stored in either 3 or 2 or 2 AZ at atime.
  So if some data lost at some AZ or any AZ is down, you still be able to access the S3 buckets.
  

S3 Storage Classes:-
  1. S3 - Standard : general-purpose storage of frequently accessed data
      Low Latancy - High Throughput -  >= 3 AZ  -
      Storage Fee:  0 till 50TB
      Retrival Fee: NA
      - Best for any general data that need frequent access.
  2. S3 IA (infrequently Access) : long-lived, but less frequently accessed data
      Low Latancy - High Throughput -  >= 3 AZ  -  
      Storage Fee:  Yes but lower then S3-Standard
      Retrival Fee: Yes
      - Best for storing data, that need to access less frequently like at a time of year end or some festival
  3. S3 IA - one zone (infrequently Access) : long-lived, but less frequently accessed data, Ok to being in service outage
      This is same as S3-IA except data lies in only one AZ. So risk if AZ goes down and cost lower
      IMP: if AZ goes down then you may face service outage for S3 bucket, but once AZ back online you again got your data and services back.
  4. S3 Inteligent Tiering: Use AI to move data b\w diffrent tiers without your intervention
      IT has two tier, Frequent access tiering And In-Frequent access tiering : -
            If data is not fetched for last 30 days, it will move in 'In-Frequent access tiering'. but as soon as it accessed
            it moved to 'Frequent access tiering'
        
  5. S3 Glacier and S3 Glacier deep Archive: for long-term archive and digital preservation
      Take minute and Hours for retrival, Glcier do not have any real time access.
    
      
Charges:
  For Storage
  For Request
  For Storage Management
  Data Transfer
  Transfer Acceleration
  Cross border replication
  
Transfer Acceleration: 
  First if you wants to use it then pay for it.
  If you wnants to accelerate the speed of UPLOADing of file on S3 bucket then use it.
  This feature actully uses the heavyweight network of edge-location and cloudFront(instead of typical internet) 
  to maximize the upload speed of the file.
When to use this -
  You have customers that upload to a centralized bucket from all over the world.
  You transfer gigabytes to terabytes of data on a regular basis across continents.
  You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3.

Cross Region replication:-
    First if you wants to use it then pay for it.
    It simply allow the bucket to automatically copy to another bucket in diffrent region. So give you kind of data security




URL of the files/object in Bcucket:-

Actully there is two kind of url generated for any file
  1. For the AWS logged in user specific 
      This is the url specifically for AWS logged-in user only, user will be able to access it only if he has proper permissions.
      Its pattern is 
      https://<Bucket Name>.s3.<Region>.amazonaws.com/<NAme of Item in Bucket>?<A larger params string>   ....#URL_1
      You can get this url from AWS > S3 > you bucket > your file > Open
      
  2. Anonymosuly : the one for the All, 
    This url is distributed all the people for accessing the file. ince they has the url they will able to access it.
    Not login or authorization needed.
    And it pattern is :-
      https://<Bucket Name>.s3.<region Name>.amazonaws.com/<NAme of Item in Bucket>  ....#URL_2
      https://  muftests3  .s3.  eu-west1   .amazonaws.com/    MufazzalPhoto

Note:
S3 is Object based storage so you can store file but not OS or database. ?????????
Also data store in form of <key> <value> pair in somewhere ???????


How to craete S3 Bucket and and upload file in it by aws portal:-
  Such Spoonfeeding! really

Folder in Bucket:-
  You can create a folder in it and its url path will in in respective of its directory path.

Once a Bucket is created, These are the operation we can do (Over the bucket not indivisual file)
  1. Enable Versioning
  2. Enable static website hosting
  3. Transfer Acceleration
  4. Create event to emit in case of any opration occure on bucket
  5. Set/Unset permission of IAM user/Group
  6. Set Bucket Policy Json
  7. Set Lifecycle (like moving to glacier after certain time or prvius verion archiving)
  8. Replication to another/same region
  ...etc
  [detail will come later]
  
  
Once a file uploaded on Bucket possible operation on File (not Bucket)
  1. Changing storage class
  2. Changing permission
  ...etc
  
  
  
Access of Bucket and its Content:-
There is three way to do it
  1. resource-based policies
      1.1 Bucket Policy
      1.2 ACL (Access control list)
  2. User Policy
    [Detail will come later] 
  
  
 Static website in S3:- 
  1. Create a bucket [mufazzalbio], mark it public access, place index.html in it
  2. myWebSiteBucket > properties > Static ebsite hosting --> Enable it and select error and index html page
  3. Though bucket is public but content in it is not so now add bucket policy like this to make all the content in the bucket public
      myWebSiteBucket > Permission > Bucket policy
      
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Sid": "PublicReadGetObject",
              "Effect": "Allow",
              "Principal": "*",
              "Action": [
                "s3:GetObject"
              ],
              "Resource": [
                "arn:aws:s3:::BUCKET_NAME/*"
              ]
            }
          ]
        }      
        
  The url for website be :-
  http://mufazzalbio.s3-website.ap-south-1.amazonaws.com
  
  page/image/file created in folder of bucket will be accessible via same relative path e.g.
    http://mufazzalbio.s3-website.ap-south-1.amazonaws.com/photo/germanyPic.png

#VVIMP: Anything you put intially in S3 in any bucket by default never be public, you have to go and set it after upload.


Alternative to AWS-S3 :-
  
  
S3 VS EBS VS EFS :-

S3: This is the Object-based storage. Each file stored in S3 has a own unique identifier or key, for access through web requests from 
      anywhere from internet or cloufrount. Also S3 support the Static Website Hosting.
      S3 is not at all the traditional file system. 
      Scalability: very high
      Latency: Low
      Its best to use for:-
      storing media content for distribution
      static website
      Archiving
 EBS: Elastic Block Syatem
      This is the Block-Storage 
      In very simple term, EBS is the drive of your EC2 instance. Every EC2 instance must have a EBS attached to it.
      Thing of this as a C or D drive of your vitual EC2 machine.
      Its fastest among all but its scalability is very dificult and required manully attachment/dettachment of EBS (Volume)
      Think of it like a local linux server with EBS as its SSD drive. If load goes up you have to stop server, add more SSD and restrat it.
      Scalability: Poor
      Latency: Lowest
      Its best for 
        Database: sql, nosql etc.
 EFS: Elastic File Syatem
      At a moment just remeber that EFS is same as EBS, But EFS can be attch to multiple EC2 instace and sevices.
      Its biggest advantage over EBS is it is auto scalable whel load fluculated ove attached EC2's.
      Scalability: b/w both
      Latency: b/w both
      
      
  Alternative of S3 ??????????????
  
  
S3 + API Getway for Upload :-
  For this you follow
    First you call a api which will verify that you have the permission for S3 upload or not. This API is coonected with any 
    lenbda authorizatuon function to determine the rights.
    This API then return the secure upload link to you if auth is successful
    Now you use this link to do the upload.
    
    Step 1
      So first create a Role which will be attached to Lembda. This role must have two policy
        step 1.1 basic lemda execution policy [choose predefined one]
        step 1.2 create one policy as such that it has pemsission to write on S3.
   Step 2
      Creste an lembda function with this role, in this lembda function handle shoul look like this-
        IMP parts- 
          UPLOAD_BUCKET_NAME: the name of bucket, should be passed as env var
          filename: the filename of uploading file
          params: ??????
          createPresignedPost: this function will generate secure url for uploading the video

          const handler = (event, context, callback) => {
                const bucket = process.env.UPLOAD_BUCKET_NAME;
                const filename = decodeURI(event.queryStringParameters.filename);
                const directory = crypto.randomBytes(20).toString('hex');
                const key = directory + '/' + filename;

                const params = { 'Bucket': bucket,
                  'Fields': {'key': key},
                  'Conditions': [{'acl': 'private'}]
                }

                s3.createPresignedPost(params, function callbackToPost(error, data) {
                  if (error) {
                     callback(null, generateResponse(400, error));
                  } else {
                    callback(null, generateResponse(200, data));
                  }
                });        
        });
        
        Now at client side use this url for uloading the vidro data.
        
        Step 3:
          Now create an api in API getway, which will expose this lembda function. It better if you also put an another lembda fuctin to check 
          user authorization. This authorizer lembda will detrmine is user is logged in and has authority to execute the lembda created in step 2/
          this authorizer do not detrmine authority to upload the file to s3 for the logged in user.
       Step 4:
           Create an API in api getway, atttach authorizer lembda of step 3 and link generater lembda of step 2 and you are done.
           
           
       
        
   ------------------------------------------
   
   Object VS block storage :-
      To understand the behavior of S3 you musr first focus on its way of working, To understnd the doffrence lets say i wants to sve a 
      100 MB file in storage.
      How diffrent type of storage will behave ? 
      1. Block based -: 
            HEre the file will be splited in small block of 1MB (just assumed) each. then this small puices are saved in memory blocks
            later when you need this 100mb file again, the storage system will retrive those 100 block, arrange them in proper order.
            recreate the 100 MB file again and return it.
            advantage - 
              1. Block storage devices provide low latency IO, so they are suitable for use by Relationshih-databases and OS.
              2. This is best if you wants to do an blob level operation eg. here you can easily append a single line to the end of a log file
             disadvatages :-
              1. if rented from any clod provider (like EC2's EBS) you will have to pay for what size you allocated.
              2. strongly tied to db and server infrastructure strongly, increasunf size of block storage may need a complete infrastrucure overhaul.
              3. You can only access block storage through a running server

        2. Object Based :-
            Here the file is not at splited, instead a file is saved as it is whithout any modification. once stoered it will return a key 
            that will be a identifier of this data or file.
            Now when you need it again you simply use this key to query and you get whole of your file back.
            In simple word - In shoping mall before you enter you deposit your beg to gatekeeper, he gives you a token. later you use this token 
              to get the bag back. The beg is an Data or file (doesnt matted the size or content of bag), the tokn is a key.
            Advangate :-
            1. The content and size of the file doesnt matter, you simply place them in storage, and get the key for later retrival.
                 It could store json, xml, image, video, js, html, mouse(ok not a mouse) almost any kind and size of file/data
            2. Its infinitly expandable, if you need to scale up/down the storage space then add/remove some storage devices and do not need to make any
               changes in rest of the infrastructure.
            3. with the key and proper HTTP api you can get anu data from storage anywhere.
            4. lot of meta data can also be saved along with file/data.
            5. A cost structure that means you only pay for what you use

            
            disadvantage :-
            1. You can’t use object storage services to back a traditional database, due to the high latency of such services
            2. Operating systems can’t easily mount an object store like a normal disk
            3. Object storage doesn’t allow you to alter just a piece of a data blob, you must read and write an entire object at once. 
                This has some performance implications. eg. to add one log line in a log file you have to delete, edit and put entire file again.
      
        
        
--------------------------------        
  
    Bucket name VS key :-
    Bucket name is assosiated with bucket and lie in universal S3 namespace.
    The 'key' belongs to the file names in that bucket.
    EG:
    https://indiaMapImages.s3.eu-west1.amazonaws.com/pune
    Here 'indiaMapImages' is the bucket name and 'pune' is the key.
    The key has to be unique in the bucket.
    
    YOu cannot have two file with same key in a bucket.
    Though you xcan make folder in bucket to use the same key name.
    EG:
    https://indiaMapImages.s3.eu-west1.amazonaws.com/pune
    and 
    https://indiaMapImages.s3.eu-west1.amazonaws.com/distric/pune
    This is two diffrent file reprset by two diffrent key 'pune' and 'distric/pune'
    
    
    --------------------------
    
    S3 security -
    S3 provide you the two type of security for ant bucket/file
    1. Encryption
    2. Access level
    
    
    
 Access Managment S3 :-
 For Access managemet there is three level where the owner can decide the access of any S3 item
  1. Bucket Policy: Ay Bucket level
  2. Access Control List: at file/object level.
    2.1. Public Accesibility of file.
    
  Bucket Policy : -
    BP is the way to define the accessibility of the bucket. The BP applied to all the object in the bucket.
    Bucket policy can grant the access of bucket for certaion operation to other user, other account, user group.
    BP attached to bucket and apply to all objevt inside.
    BP cannot be attached to object inside.
    This is simplay a json doc.
    
    
 Access Control List :- 
    The BP is applied over the Bucket, that give you quite higher level of access management
    The ACL is applied over the object inside the bucket. It define what user, user group, etc can have access to 
    the underline object [not bucket].
    It gives you very fine grain way to manage access of any objct in S3
   
 2.1  
 Anonymus Public Accesibility [Special cass of ACL] :-    
   The APA provide tou a extremly simpole way to make any file/object public, without digging into ACL.
   It automatically manage/create/edit the attached ACL to make the file/objct anonymously accessible with #URL_2.
   The first two kind of Access management (ACL and BP) tells you that if user/user group/account/role etc can do certaion operration
   on the underline resource or not
   APA will set up the ACL properly to make the file be accessible ANONYMOuSLY or not. mns will this file be accessible with #URL_2 or not.
   VVIMP: You must stuck in your mind that 'Anonymus Public Accesibility' only and only deal with url '#URL_2'
   How to set it:-
    1.  For new upload: At a time of upload at permission page just chose 'Manage public permissions' to 'grant public read access'
        [There is only 2 option to choose 'grant public read access' and 'do not grant public read access']
    2.  For existing one: AWS > S3 > 'your bucket' > 'your file' > Action > Make Public
     >> both way will actully configure the ACL behind the scene
    
    
Block Public Access feature :-        
  Above all 3 there is one more option lies, and this can (or cannot) override all the permission granted through either of above three
  this is 'Block Public Access feature'.
  This has power to deny the access granted through above 3, but do not have any way to grant acceess which is deny by above 3
  This has a 4 option that you can select at a time of either bucket creation or from   
        AWS > S3 > your bucket > Permisssion >  Block Public Access >  4 checkboxes. 
  
  If you have checked all 4:-
    It means you have blocked all the access to the public. this will completely override all the accesses granted throug 
    either BP, ACL, APA. All the access are denied. ?????????????i doubt?????????
    
 if all 4 are unchecked :-
    In this case this permission filter is completely shut off. it wont deny any kind of access. So all the permission granted through
    either BP, ACL, APA will be honoured. ?????????????i doubt?????????

  Default value :-
    By default the all 4 remain checked, so the bucket by default deny any kinfd on bublic access.    
      
      
 Block-Public-Access-feature and APA relation :-
      When you set APA = deny/grant, you actully change the ACL of underline file/object.
      Now you have 4 chack box in BPAF, 2 for BP and 2 for ACL.
      So
      The access granted by ACL/APA could be overriden by BPAF, if the two check box of BAPF belongs to ACL are checked.
 
      If you have checked 2 ACL checkbox of 'Block-Public-Access-feature':-
        It means you have blocked all the Anonymuus public access granted via APA. this will completely override all the accesses granted throug APA.
        So #urL_2 will not work  
          
     if all 4 are unchecked of 'Block-Public-Access-feature' :-
        In this case this permission filter is completely shut off. So all the permission granted through APA will be honoured.   
        So #urL_2 will work
        
     Scenarion #1: 
      You have a bucket where Block-Public-Access-feature = 2 ACL un-checked (so block all public access via ACL turned off). 
      and you have many files/object in this bucket that are either have APA = granted or APA = denied
      So what happed if I change 
          Block-Public-Access-feature = 2-ACL checked  (so block all public access via ACL turned on). 
      - Here the 'Block-Public-Access-feature' will override the permisssions granted through APA. and so no one will be able to read the #URL_2
     Scenario#2: 
      You have a bucket where Block-Public-Access-feature = 2 ACL un-checked (so block all public access via ACL turned off). 
      So what happed if I upload a and set APA = granted/denied ? 
      Both case are fine, AWS system will let you do it. No error will come.
    Scenario#3:
      You have a bucket where Block-Public-Access-feature = 2 ACL checked (so block all public access via ACL turned on). 
      So what happed if I upload a and set APA = granted/denied ? 
      Only APA = denied, will work granted will give error, that is because you cannot upload the file with publicaly accessible setting 
      when the bucket itself deny any public access.
      Also what happen if i change the existing file/obect to APA = granted, The option will remain disabled actully.
      You cannot set the file APA to publically accessible if bucket deny any public access.
      

        
    
    
    
    
    


