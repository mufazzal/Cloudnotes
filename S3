In very Simple word S3 (Simple Storage Service) is STATIC STORAGE. 
A bucket that is best place to put static HTML, css, video, image, static JS  etc.

Things that should be stored in S3 :-
  Any kind of flat file like image, .html. video etc
Wich should not:-
  Any dynamic/processable/database/OS file
  
 
Bucket:-
  In very simple word, This is a kind of UNIQUE folder in the ENTIRE S3 UNIVERSE
  So when you create it, its name must be quique (not relative to the buckets in your S3 account but in an entire S3 unviverse)
  So you can not take name like test, test123 etc as they may have taken centuries ago. 

Region of S3:-
  S3 is region independent, when you open S3 in aws console you will see region selected as 'Global' and other rigion remian unselectable.
  It means that S3 service remain same in entire aws system.
  If "Test" bucket in created in region "Asia" then "Test" cant be created again by anyone in any region ever.
Region of bucket:
  Though S3 is region independent, but the bucket created in S3 by you will must lie in a selected region 
  
Q:  Can i access the bucket on region A in Region B
  ????????
 

How To create Bucket :-
  1. aws > S3 > Create Bucket
  2. Fill Name: Mut be unique in all S3 universe 
  3. Select Region: You should select nearest one (VVIMP: Though you select region here but still the name should ne unique to entire S3 universe )
  4. Couple of more thing (will be covered later)


S3 and Data Consistency :-
1: Read after Write: its consistent, mns you will be able to read the file immdietly after writing it.
2: Read after Delete/Change: This may take some time to reflect, so if you read immedietly after then you may get the previouse version



S3 and Region :-
  When you create a bucket youu have to select the region, and so toyr bucket will lie in that region. 
  VVIMP: it not at all means that content of this bucket wont be avialabler in other region.
  like you create a bucket 'indiaMapImages' in mumbai. 
  but the url 'https:// indiaMapImages.s3.eu-west1.amazonaws.com/pune' will be accessible in anywhere in the world.
  Though the time taken to load the content in newyork will be greater than in mumbai.


S3 and AZ :- 
  Bucket in region specific, you do not have to choose the AZ at all when you create bucket.
  Depending on your storage class, the content of the bucket will be stored in either 3 or 2 or 2 AZ at atime.
  So if some data lost at some AZ or any AZ is down, you still be able to access the S3 buckets.
  

S3 Storage Classes:-
  1. S3 - Standard : general-purpose storage of frequently accessed data
      Low Latancy - High Throughput -  >= 3 AZ  -
      Storage Fee:  0 till 50TB
      Retrival Fee: NA
      - Best for any general data that need frequent access.
  2. S3 IA (infrequently Access) : long-lived, but less frequently accessed data
      Low Latancy - High Throughput -  >= 3 AZ  -  
      Storage Fee:  Yes but lower then S3-Standard
      Retrival Fee: Yes
      - Best for storing data, that need to access less frequently like at a time of year end or some festival
  3. S3 IA - one zone (infrequently Access) : long-lived, but less frequently accessed data, Ok to being in service outage
      This is same as S3-IA except data lies in only one AZ. So risk if AZ goes down and cost lower
      IMP: if AZ goes down then you may face service outage for S3 bucket, but once AZ back online you again got your data and services back.
  4. S3 Inteligent Tiering: Use AI to move data b\w diffrent tiers without your intervention
      IT has two tier, Frequent access tiering And In-Frequent access tiering : -
            If data is not fetched for last 30 days, it will move in 'In-Frequent access tiering'. but as soon as it accessed
            it moved to 'Frequent access tiering'
        
  5. S3 Glacier and S3 Glacier deep Archive: for long-term archive and digital preservation
      Take minute and Hours for retrival, Glcier do not have any real time access.
    
      
Charges:
  For Storage
  For Request
  For Storage Management
  Data Transfer
  Transfer Acceleration
  Cross border replication
  
Transfer Acceleration: 
  First if you wants to use it then pay for it.
  If you wnants to accelerate the speed of UPLOADing of file on S3 bucket then use it.
  This feature actully uses the heavyweight network of edge-location and cloudFront(instead of typical internet) 
  to maximize the upload speed of the file.
When to use this -
  You have customers that upload to a centralized bucket from all over the world.
  You transfer gigabytes to terabytes of data on a regular basis across continents.
  You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3.

Cross Region replication:-
    First if you wants to use it then pay for it.
    It simply allow the bucket to automatically copy to another bucket in diffrent region. So give you kind of data security




URL of the cucket:-
  A url of the bucket is unique for each indivisual bucket in entire S3 universe
And it pattern is :-
  https://<Bucket Name>.s3.<region Name>.amazonaws.com/<NAme of Item in Bucket>
  https://  muftests3  .s3.  eu-west1   .amazonaws.com/    MufazzalPhoto

Note:
S3 is Object based storage so you can store file but not OS or database. ?????????
Also data store in form of <key> <value> pair in somewhere ???????


How to craete S3 Bucket and and upload file in it by aws portal:-
  Such Spoonfeeding! really

Folder in Bucket:-
  You can create a folder in it and its url path will in in respective of its directory path.

Once a Bucket is created, These are the operation we can do (Over the bucket not indivisual file)
  1. Enable Versioning
  2. Enable static website hosting
  3. Transfer Acceleration
  4. Create event to emit in case of any opration occure on bucket
  5. Set/Unset permission of IAM user/Group
  6. Set Bucket Policy Json
  7. Set Lifecycle (like moving to glacier after certain time or prvius verion archiving)
  8. Replication to another/same region
  ...etc
  [detail will come later]
  
  
Once a file uploaded on Bucket possible operation on File (not Bucket)
  1. Changing storage class
  2. Changing permission
  ...etc
  
  
  
Access of Bucket and its Content:-
There is three way to do it
  1. resource-based policies
      1.1 Bucket Policy
      1.2 ACL (Access control list)
  2. User Policy
    [Detail will come later] 
  
  
 Static website in S3:- 
  1. Create a bucket [mufazzalbio], mark it public access, place index.html in it
  2. myWebSiteBucket > properties > Static ebsite hosting --> Enable it and select error and index html page
  3. Though bucket is public but content in it is not so now add bucket policy like this to make all the content in the bucket public
      myWebSiteBucket > Permission > Bucket policy
      
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Sid": "PublicReadGetObject",
              "Effect": "Allow",
              "Principal": "*",
              "Action": [
                "s3:GetObject"
              ],
              "Resource": [
                "arn:aws:s3:::BUCKET_NAME/*"
              ]
            }
          ]
        }      
        
  The url for website be :-
  http://mufazzalbio.s3-website.ap-south-1.amazonaws.com
  
  page/image/file created in folder of bucket will be accessible via same relative path e.g.
    http://mufazzalbio.s3-website.ap-south-1.amazonaws.com/photo/germanyPic.png

#VVIMP: Anything you put intially in S3 in any bucket by default never be public, you have to go and set it after upload.


Alternative to AWS-S3 :-
  
  
S3 VS EBS VS EFS :-

S3: This is the Object-based storage. Each file stored in S3 has a own unique identifier or key, for access through web requests from 
      anywhere from internet or cloufrount. Also S3 support the Static Website Hosting.
      S3 is not at all the traditional file system. 
      Scalability: very high
      Latency: Low
      Its best to use for:-
      storing media content for distribution
      static website
      Archiving
 EBS: Elastic Block Syatem
      This is the Block-Storage 
      In very simple term, EBS is the drive of your EC2 instance. Every EC2 instance must have a EBS attached to it.
      Thing of this as a C or D drive of your vitual EC2 machine.
      Its fastest among all but its scalability is very dificult and required manully attachment/dettachment of EBS (Volume)
      Think of it like a local linux server with EBS as its SSD drive. If load goes up you have to stop server, add more SSD and restrat it.
      Scalability: Poor
      Latency: Lowest
      Its best for 
        Database: sql, nosql etc.
 EFS: Elastic File Syatem
      At a moment just remeber that EFS is same as EBS, But EFS can be attch to multiple EC2 instace and sevices.
      Its biggest advantage over EBS is it is auto scalable whel load fluculated ove attached EC2's.
      Scalability: b/w both
      Latency: b/w both
      
      
  Alternative of S3 ??????????????
  
  
S3 + API Getway for Upload :-
  For this you follow
    First you call a api which will verify that you have the permission for S3 upload or not. This API is coonected with any 
    lenbda authorizatuon function to determine the rights.
    This API then return the secure upload link to you if auth is successful
    Now you use this link to do the upload.
    
    Step 1
      So first create a Role which will be attached to Lembda. This role must have two policy
        step 1.1 basic lemda execution policy [choose predefined one]
        step 1.2 create one policy as such that it has pemsission to write on S3.
   Step 2
      Creste an lembda function with this role, in this lembda function handle shoul look like this-
        IMP parts- 
          UPLOAD_BUCKET_NAME: the name of bucket, should be passed as env var
          filename: the filename of uploading file
          params: ??????
          createPresignedPost: this function will generate secure url for uploading the video

          const handler = (event, context, callback) => {
                const bucket = process.env.UPLOAD_BUCKET_NAME;
                const filename = decodeURI(event.queryStringParameters.filename);
                const directory = crypto.randomBytes(20).toString('hex');
                const key = directory + '/' + filename;

                const params = { 'Bucket': bucket,
                  'Fields': {'key': key},
                  'Conditions': [{'acl': 'private'}]
                }

                s3.createPresignedPost(params, function callbackToPost(error, data) {
                  if (error) {
                     callback(null, generateResponse(400, error));
                  } else {
                    callback(null, generateResponse(200, data));
                  }
                });        
        });
        
        Now at client side use this url for uloading the vidro data.
        
        Step 3:
          Now create an api in API getway, which will expose this lembda function. It better if you also put an another lembda fuctin to check 
          user authorization. This authorizer lembda will detrmine is user is logged in and has authority to execute the lembda created in step 2/
          this authorizer do not detrmine authority to upload the file to s3 for the logged in user.
       Step 4:
           Create an API in api getway, atttach authorizer lembda of step 3 and link generater lembda of step 2 and you are done.
           
           
       
        
   ------------------------------------------
   
   Object VS block storage :-
      To understand the behavior of S3 you musr first focus on its way of working, To understnd the doffrence lets say i wants to sve a 
      100 MB file in storage.
      How diffrent type of storage will behave ? 
      1. Block based -: 
            HEre the file will be splited in small block of 1MB (just assumed) each. then this small puices are saved in memory blocks
            later when you need this 100mb file again, the storage system will retrive those 100 block, arrange them in proper order.
            recreate the 100 MB file again and return it.
            advantage - 
              1. Block storage devices provide low latency IO, so they are suitable for use by Relationshih-databases and OS.
              2. This is best if you wants to do an blob level operation eg. here you can easily append a single line to the end of a log file
             disadvatages :-
              1. if rented from any clod provider (like EC2's EBS) you will have to pay for what size you allocated.
              2. strongly tied to db and server infrastructure strongly, increasunf size of block storage may need a complete infrastrucure overhaul.
              3. You can only access block storage through a running server

        2. Object Based :-
            Here the file is not at splited, instead a file is saved as it is whithout any modification. once stoered it will return a key 
            that will be a identifier of this data or file.
            Now when you need it again you simply use this key to query and you get whole of your file back.
            In simple word - In shoping mall before you enter you deposit your beg to gatekeeper, he gives you a token. later you use this token 
              to get the bag back. The beg is an Data or file (doesnt matted the size or content of bag), the tokn is a key.
            Advangate :-
            1. The content and size of the file doesnt matter, you simply place them in storage, and get the key for later retrival.
                 It could store json, xml, image, video, js, html, mouse(ok not a mouse) almost any kind and size of file/data
            2. Its infinitly expandable, if you need to scale up/down the storage space then add/remove some storage devices and do not need to make any
               changes in rest of the infrastructure.
            3. with the key and proper HTTP api you can get anu data from storage anywhere.
            4. lot of meta data can also be saved along with file/data.
            5. A cost structure that means you only pay for what you use

            
            disadvantage :-
            1. You can’t use object storage services to back a traditional database, due to the high latency of such services
            2. Operating systems can’t easily mount an object store like a normal disk
            3. Object storage doesn’t allow you to alter just a piece of a data blob, you must read and write an entire object at once. 
                This has some performance implications. eg. to add one log line in a log file you have to delete, edit and put entire file again.
      
        
        
--------------------------------        
  
    Bucket name VS key :-
    Bucket name is assosiated with bucket and lie in universal S3 namespace.
    The 'key' belongs to the file names in that bucket.
    EG:
    https://indiaMapImages.s3.eu-west1.amazonaws.com/pune
    Here 'indiaMapImages' is the bucket name and 'pune' is the key.
    The key has to be unique in the bucket.
    
    YOu cannot have two file with same key in a bucket.
    Though you xcan make folder in bucket to use the same key name.
    EG:
    https://indiaMapImages.s3.eu-west1.amazonaws.com/pune
    and 
    https://indiaMapImages.s3.eu-west1.amazonaws.com/distric/pune
    This is two diffrent file reprset by two diffrent key 'pune' and 'distric/pune'
    
    
    --------------------------
    
    S3 security -
    1. Default Access :-
        By default the Bucket is private and only the owner who created the bucket can do any operation be it read, write, delete, edit.
        No onle else has any access to it.
        So by default the bucket is not public
        
   Bucket Policy : -
    BP is the way to define the accessibility of the bucket. The BP applied to all the object in the bucket.
    Bucket policy can grant the access of bucket for certaion operation to other user, other account, user group.
    BP attached to bucket and apply to all objevt inside.
    BP cannot be attached to object inside.
    This is simplay a json doc.
    
    
   Access Control List :- 
      The BP is applied over the Bucket, that give you quite higher level of access management
      The ACL is applied over the object inside the bucket. It define what user, user group, etc can have access to 
      the underline object [not bucket].
      It gives you very fine grain way to manage access of any objct in S3
    
    
    
    
    
    


