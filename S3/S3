In very Simple word S3 (Simple Storage Service) is STATIC STORAGE. 
A bucket that is best place to put static HTML, css, video, image, static JS  etc.

Things that should be stored in S3 :-
  Any kind of flat file like image, .html. video etc
Wich should not:-
  Any dynamic/processable/database/OS file
  
S3 is Object based storage so you can store file but not OS or database. ?????????
Also data store in form of <key> <value> pair in somewhere ??????? 
 
Bucket:-
  In very simple word, This is a kind of UNIQUE folder in the ENTIRE S3 UNIVERSE
  So when you create it, its name must be quique (not relative to the buckets in your S3 account but in an entire S3 unviverse)
  So you can not take name like test, test123 etc as they may have taken centuries ago. 

Region of S3:-
  S3 is region independent, when you open S3 in aws console you will see region selected as 'Global' and other rigion remian unselectable.
  It means that S3 service remain same in entire aws system.
  If "Test" bucket in created in region "Asia" then "Test" cant be created again by anyone in any region ever.
Region of bucket:
  Though S3 is region independent, but the bucket created in S3 by you will must lie in a selected region 
  When you create a bucket you have to select the region, and so your bucket will lie in that region. 
  VVIMP: it not at all means that content of this bucket wont be avialabler in other region.
  like you create a bucket 'indiaMapImages' in mumbai. 
  but the url 'https:// indiaMapImages.s3.eu-west1.amazonaws.com/pune' will be accessible in anywhere in the world.
  Though the time taken to load the content in newyork will be greater than in mumbai. 
S3 and AZ :- 
  Bucket in region specific, you do not have to choose the AZ at all when you create bucket.
  Depending on your storage class, the content of the bucket will be stored in either 3 or 2 or 2 AZ at atime.
  So if some data lost at some AZ or any AZ is down, you still be able to access the S3 buckets.

S3 has thhe flate file structure. thoug it apear to be have a directiris inside the bucket but s3 actully store it in flt system.
EG:-
  http://s3.mufbucket......com/folder1/folder/2/folder3/image.jpg  
  It does not mean that there is 3 inner folder. It actully mean that a object with key 'folder1/folder/2/folder3/image.jpg'
  For easier viewing experience AWS console also do the logical nesting based on '/' in object key.
  
How To create Bucket :-
  1. aws > S3 > Create Bucket
  2. Fill Name: Mut be unique in all S3 universe 
  3. Select Region: You should select nearest one (VVIMP: Though you select region here but still the name should ne unique to entire S3 universe )
  4. Couple of more thing (will be covered later)


S3 and Data Consistency :-
  on DEC 2020 AWS madr theagic happen. now PUT, DELETE, overwrite PUT(update), 
  all those operation are strongly consistent for GET and LIST operation.
  After a successful write of a 
    new object, or 
    an overwrite or 
    delete of an existing object, 
  any subsequent GET or LIST request immediately receives the latest version of the object.
  
  The only Eventual Consistent is 'Bucket Delete'. You might see the bucket in the 'ListBucket' for few moments after deleting the bucket.
----------------------------
  

S3 Storage Classes:-
  Storage classes are applied to a Indivisual Object not on Bucket
  1. S3 - Standard : general-purpose storage of frequently accessed data
      Low Latancy - High Throughput -  >= 3 AZ  -
      Storage Fee:  0 till 50TB
      Retrival Fee: NA
      - Best for any general data that need frequent access.
  2. S3 Standard_IA (infrequently Access) : long-lived, but less frequently accessed data
      Low Latancy - High Throughput -  >= 3 AZ  -  all same as standard.
      Storage Fee:  Yes but lower then S3-Standard
      Retrival Fee: Yes
      S3 Standard-IA costs less than S3 Standard in terms of storage price, while still providing the same high durability, throughput, and low latency of S3 Standard.
      But S3 Standard-IA have cost for data retrieval.
      unlike Galcier it gives you rapid access to the file.
      Minimum retention priode and objecte size is 30 days and 128kb. mns even you store a 5kb object for 10 days, u will be charged with 128kb for 30 days. 
      - Best for storing data, that need to access less frequently like at a time of year end or some festival
  3. S3 Standard_IA_one_zone (infrequently Access) : long-lived, but less frequently accessed data, Ok to being in service outage
      This is same as S3-IA except data lies in only one AZ. So risk if AZ goes down and cost lower
      IMP: if AZ goes down then you may face service outage for S3 bucket, but once AZ back online you again got your data and services back.
      Minimum retention priode and objecte size is 30 days and 128kb. mns even you store a 5kb object for 10 days, u will be charged with 128kb for 30 days. 
  4. S3 Inteligent Tiering: Use AI to move data b\w diffrent tiers without your intervention
      IT has two tier, Frequent access tiering And In-Frequent access tiering : -
            If data is not fetched for last 30 days, it will move in 'In-Frequent access tiering'. but as soon as it accessed
            it moved to 'Frequent access tiering'
      Storage Fee:  Yes but very efficient
      Retrival Fee: No but charged for data movement between tiering.
      Minimum retention priode is 30 days. mns even you store a object for 10 days, u will be charged for 30 d
            
  5. S3 Glacier and S3 Glacier deep Archive: for long-term archive and digital preservation
      Take minute and Hours for retrival, Glcier do not have any real time access.
      You will be charged minimum for 90 days and 40 KB
      
    
-------------------------
Limits:-  
  Default Max No of bucket you can make in AWS account
    - 100, for increasing limit contact to aws service center.
  No of obect in a bucket
    - Infinite
  MAx size of a object that can be uploaded
    - 5 TB
  MAx tag on object
    - 10
  Max limit for PUT:
    - 5GB
    
--------------------   
Charges:
  For Storage
  For Request
  Data Transfer out od S3 (into s3 is free)
  For Storage Management
  Data Transfer
  Transfer Acceleration
  Cross border replication
  Object Tagging
  
-----------------

URL of the files/object in Bcucket:-
  There is 3 diffrent URL pattern to access the S3 object
  
  Virtual region specific
      https://<Bucket Name>.s3.<region Name>.amazonaws.com/<Object Key>  ....#URL_2
  Virtual Global
      https://<Bucket Name>.s3.amazonaws.com/<Object Key>  ....#URL_2
  Path
      https://s3-<region Name>.amazonaws.com/<Bucket Name>/<Object Key>  ....#URL_3
        This style is going to be deprecated.


-------------------------

Metadata:- 
  S3 also give you the way to store data bout data. EG content-type, content-lengh etc. 
  There is basicaly two type of metadata
  1. System MEta data:
      Date, Content-Length, Content-Type
      x-amz-server-side-encryption, x-amz-version-id, x-amz-storage-class	etc.
  2. User defined meta:
      define with header 'x-amz-meta-<name of meta data>'
      
  IMP: S3 do not encrypt MEtadat even if encryption is enabled so never store sensitive info in it
  
  Tag:- You can also attach the tag on the Object. 

--------------------------

Object Taging:-
  You cann attach deattach the Tag on Objevt. with the help of tagging you can do below things 
    1. Fine grain access control
    2. Fine grained Lifcycle managment
    3. Cloud watch and cloudtrain metrics filtering

-------------------------
  
 Static website in S3:- 
  1. Create a bucket [mufazzalbio], mark it public access, place index.html in it
  2. myWebSiteBucket > properties > Static ebsite hosting --> Enable it and select error and index html page
  3. Though bucket is public but content in it is not so now add bucket policy like this to make all the content in the bucket public
      myWebSiteBucket > Permission > Bucket policy
      
        { "Version": "2012-10-17",
          "Statement": [{
              "Effect": "Allow",
              "Principal": "*",
              "Action": [ "s3:GetObject" ],
              "Resource": [ "arn:aws:s3:::BUCKET_NAME/*"]
            }]
        }      
        
  The url for website be :-
    http://<<bucket-name>>.s3-website.<<Region>>.amazonaws.com
    http://<<bucket-name>>.s3-website-<<Region>>.amazonaws.com
  
  page/image/file created in folder of bucket will be accessible via same relative path e.g.
    http://mufazzalbio.s3-website.ap-south-1.amazonaws.com/photo/germanyPic.png

#VVIMP: Anything you put intially in S3 in any bucket by default never be public, you have to go and set it after upload.

      
--------------------
    
S3 Bucket and CORS:- To get any objevt from S3 we generally use GetObject web-service, now if you call it direcl=tly from 
   browser URL or from Postman, it work fine but if we call it via any other origin eg: localhost.com or www.mufsite.com, 
   or by fetch(..) function of JS, then you may gey the CORS error.
   TO resolve this just add the origin To the bucket it lies in.
   aws > s3 > 'your bucket' > permission > CORS.
   In each CORWS rule you need to specifu three thing:-
    AllowedMethods: 
     PUT, POST, DELETE, GET,HEAD. [If CORS is enabled then OPTION is allowed bu default]
    AllowedOrigin:-
      This list of domains to be allowed.
    AlloweHeader:-
      The header that are allowed in Perflight OPTION Request.
    ExposeHeader:-
      The header that will be redable by called application client.
    MaxAgeSeconds:
      The time how long the response Perflight OPTION call will be cached.

   Note: The CORS config is set over the Bucket not on the Object
       
--------------------

  S3 Signed URLs:-
    Signed url provied you an pre-authenticated url wich is generated via creds of some authorized user. this url can be used used for 2 of operation on S3
    1. GET: read/download object
    2. PUT: upload/update the Object.
    for creating signed url you will need below ingredient-
      Access Id, Secret key, Bucket name, Object name, operation type(getObject, putObject), expiration time
    When you may need this:-
      - You wants to give someone a url to download/upload the object without any creds. 
        the reciver of this url will be able to download/upload the object anywhere without any login.
      - by s3.upload you get the object/file in binary formate (like response: {data: Array[0-100, 101-1000, ...]}) and you need to convert this binary data in strean or file
        by signed url mechanism you can get url to download this file directly.
    EG:-
            AWS.config.update({ credentials: new AWS.Credentials({ accessKeyId: ..., secretAccessKey: .... }) });
            var s3 = new AWS.S3({apiVersion: '2006-03-01'});

            var params = {
              Bucket: document.getElementById("pBucketName").value,
              Key: document.getElementById("objKey").value,
              Expires: 60
            };
            var url = s3.getSignedUrl('getObject', params);
    Acceess and signedUrl:-
      To create the signed url you do not need any permission. but this url will work only if the cred used in the creation process has the proped permission to do operation.
      otherwise user will get the Access denied on that url.

---------------------------------------------
  S3 partition:
    ?????????????//
  How to upload a very large file of < 1TB
     
-------------------------

TODO:-
  S3 Select
  VPC S3 endpoint
  Object Lifecycle set up
  S3 Multipart
  S3 AWS JS SDK
  
  



