In very Simple word S3 (Simple Storage Service) is STATIC STORAGE. 
A bucket that is best place to put static HTML, css, video, image, static JS  etc.

Things that should be stored in S3 :-
  Any kind of flat file like image, .html. video etc
Wich should not:-
  Any dynamic/processable/database/OS file
  
S3 is Object based storage so you can store file but not OS or database. ?????????
Also data store in form of <key> <value> pair in somewhere ??????? 
 
Bucket:-
  In very simple word, This is a kind of UNIQUE folder in the ENTIRE S3 UNIVERSE
  So when you create it, its name must be quique (not relative to the buckets in your S3 account but in an entire S3 unviverse)
  So you can not take name like test, test123 etc as they may have taken centuries ago. 

Region of S3:-
  S3 is region independent, when you open S3 in aws console you will see region selected as 'Global' and other rigion remian unselectable.
  It means that S3 service remain same in entire aws system.
  If "Test" bucket in created in region "Asia" then "Test" cant be created again by anyone in any region ever.
Region of bucket:
  Though S3 is region independent, but the bucket created in S3 by you will must lie in a selected region 
 

How To create Bucket :-
  1. aws > S3 > Create Bucket
  2. Fill Name: Mut be unique in all S3 universe 
  3. Select Region: You should select nearest one (VVIMP: Though you select region here but still the name should ne unique to entire S3 universe )
  4. Couple of more thing (will be covered later)


S3 and Data Consistency :-
1: Read after Write for PUT: its consistent, mns you will be able to read the file immdietly after writing it. So when upload a new object the upload reuest finish 
        when object is uploaded to all the destination with succes otherwise the upload wil fail and you will not be able to read after PUT.
        So after successful PUT you will surely get the data
2: Eventual Consistency for UPDATE/DELETE: This may take some time to reflect, so if you read immedietly after then you may get the previouse version. So when you delete or update the 
        objevt then the request finish even when the object is not yet update/deleted to all destinations. 
        So after successful DELETE/UPDATE you may or may not get the new data


S3 and Region :-
  When you create a bucket youu have to select the region, and so toyr bucket will lie in that region. 
  VVIMP: it not at all means that content of this bucket wont be avialabler in other region.
  like you create a bucket 'indiaMapImages' in mumbai. 
  but the url 'https:// indiaMapImages.s3.eu-west1.amazonaws.com/pune' will be accessible in anywhere in the world.
  Though the time taken to load the content in newyork will be greater than in mumbai.


S3 and AZ :- 
  Bucket in region specific, you do not have to choose the AZ at all when you create bucket.
  Depending on your storage class, the content of the bucket will be stored in either 3 or 2 or 2 AZ at atime.
  So if some data lost at some AZ or any AZ is down, you still be able to access the S3 buckets.
  
-----------------------------


S3 Storage Classes:-
  Storage classes are applied to a Indivisual Object not on Bucket
  1. S3 - Standard : general-purpose storage of frequently accessed data
      Low Latancy - High Throughput -  >= 3 AZ  -
      Storage Fee:  0 till 50TB
      Retrival Fee: NA
      - Best for any general data that need frequent access.
  2. S3 Standard_IA (infrequently Access) : long-lived, but less frequently accessed data
      Low Latancy - High Throughput -  >= 3 AZ  -  
      Storage Fee:  Yes but lower then S3-Standard
      Retrival Fee: Yes
      Minimum retention priode and objecte size is 30 days and 128kb. mns even you store a 5kb object for 10 days, u will be charged with 128kb for 30 days. 
      - Best for storing data, that need to access less frequently like at a time of year end or some festival
  3. S3 Standard_IA_one_zone (infrequently Access) : long-lived, but less frequently accessed data, Ok to being in service outage
      This is same as S3-IA except data lies in only one AZ. So risk if AZ goes down and cost lower
      IMP: if AZ goes down then you may face service outage for S3 bucket, but once AZ back online you again got your data and services back.
  4. S3 Inteligent Tiering: Use AI to move data b\w diffrent tiers without your intervention
      IT has two tier, Frequent access tiering And In-Frequent access tiering : -
            If data is not fetched for last 30 days, it will move in 'In-Frequent access tiering'. but as soon as it accessed
            it moved to 'Frequent access tiering'
      Storage Fee:  Yes but very efficient
      Retrival Fee: No but charged for data movement between tiering.
      Minimum retention priode is 30 days. mns even you store a object for 10 days, u will be charged for 30 days. 
            
  5. S3 Glacier and S3 Glacier deep Archive: for long-term archive and digital preservation
      Take minute and Hours for retrival, Glcier do not have any real time access.
      
    
--------------------
Lifecycle Policy:-
  The life cycle policy is apply to a group of object[all in folder] or on whole bucket.
  In lifecycle policy the user define the set of configuration to move the objects in diffrent storage class.
  This is best if you know the lifecycle of the object.
  The is two rule you can define
  Transition Rule:- Rule to determine when and which storage class the object should move
  Expiration rule: set when to delete the object.

Lifecycle Policy VS Inteligent Tiering
  Inteligent Tiering use the amazon developed algorith to move the object between the classes. In Lifecycle Policy the developer himself provide the set of rule to do this.
  EG:-  in Inteligent Tiering the object moves between standard to stanrard_IA in 30 days. in lifecycle you can do set you own rule of time, date, size etc.

-------------------------
Limits:-  
  Default Max No of bucket you can make in AWS account
    - 100, for increasing limit contact to aws service center.
  No of obect in a bucket
    - Infinite
  MAx size of a object that can be uploaded
    - 5 TB
  MAx tag on object
    - 10
    
   
Charges:
  For Storage
  For Request
  Data Transfer out od S3 (into s3 is free)
  For Storage Management
  Data Transfer
  Transfer Acceleration
  Cross border replication
  Object Tagging
  
Transfer Acceleration: 
  First if you wants to use it then pay for it.
  If you wnants to accelerate the speed of UPLOADing of file on S3 bucket then use it.
  This feature actully uses the heavyweight network of edge-location and cloudFront(instead of typical internet) 
  to maximize the upload speed of the file.
When to use this -
  You have customers that upload to a centralized bucket from all over the world.
  You transfer gigabytes to terabytes of data on a regular basis across continents.
  You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3.

Cross Region replication:-
    First if you wants to use it then pay for it.
    It simply allow the bucket to automatically copy to another bucket in diffrent region. So give you kind of data security


URL of the files/object in Bcucket:-
  There is 3 diffrent URL pattern to access the S3 object
  
  Virtual region specific
      https://<Bucket Name>.s3.<region Name>.amazonaws.com/<Object Key>  ....#URL_2
  Virtual Global
      https://<Bucket Name>.s3.amazonaws.com/<Object Key>  ....#URL_2
  Path
      https://s3-<region Name>.amazonaws.com/<Bucket Name>/<Object Key>  ....#URL_3
        This style is going to be deprecated.


---------------------

Object Versioning:-
  You can enable the versioning on any object in S3. By this you can keep all the older vesion of that S3 object.
  The versioning is enabled over the object not bucket.
  So 
  Bucket-Name + Object-key + Version gives you the unique s3 object in S3.

-------------------------

Metadata:- S3 also give you the way to store data bout data. EG content-type, content-lengh etc.
Tag:- You can also attach the tag on the Object. 

--------------------------

S3 has thhe flate file structure. thoug it apear to be have a directiris inside the bucket but s3 actully store it in flt system.
EG:-
  http://s3.mufbucket......com/folder1/folder/2/folder3/image.jpg  
  It does not mean that there is 3 inner folder. It actully mean that a object with key 'folder1/folder/2/folder3/image.jpg'
  For easier viewing experience AWS console also do the logical nesting based on '/' in object key.

--------------------------

Object Taging:-
  You cann attach deattach the Tag on Objevt. with the help of tagging you can do below things 
    1. Fine grain access control
    2. Fine grained Lifcycle managment
    3. Cloud watch and cloudtrain metrics filtering

-------------------------
How to craete S3 Bucket and and upload file in it by aws portal:-
  Such Spoonfeeding! really

Folder in Bucket:-
  You can create a folder in it and its url path will in in respective of its directory path.

Once a Bucket is created, These are the operation we can do (Over the bucket not indivisual file)
  1. Enable Versioning
  2. Enable static website hosting
  3. Transfer Acceleration
  4. Create event to emit in case of any opration occure on bucket
  5. Set/Unset permission of IAM user/Group
  6. Set Bucket Policy Json
  7. Set Lifecycle (like moving to glacier after certain time or prvius verion archiving)
  8. Replication to another/same region
  ...etc
  [detail will come later]
  
  
Once a file uploaded on Bucket possible operation on File (not Bucket)
  1. Changing storage class
  2. Changing permission
  ...etc
  
  
  
Access of Bucket and its Content:-
There is three way to do it
  1. resource-based policies
      1.1 Bucket Policy
      1.2 ACL (Access control list)
  2. User Policy
    [Detail will come later] 
  
  
 Static website in S3:- 
  1. Create a bucket [mufazzalbio], mark it public access, place index.html in it
  2. myWebSiteBucket > properties > Static ebsite hosting --> Enable it and select error and index html page
  3. Though bucket is public but content in it is not so now add bucket policy like this to make all the content in the bucket public
      myWebSiteBucket > Permission > Bucket policy
      
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Sid": "PublicReadGetObject",
              "Effect": "Allow",
              "Principal": "*",
              "Action": [
                "s3:GetObject"
              ],
              "Resource": [
                "arn:aws:s3:::BUCKET_NAME/*"
              ]
            }
          ]
        }      
        
  The url for website be :-
  http://mufazzalbio.s3-website.ap-south-1.amazonaws.com
  
  page/image/file created in folder of bucket will be accessible via same relative path e.g.
    http://mufazzalbio.s3-website.ap-south-1.amazonaws.com/photo/germanyPic.png

#VVIMP: Anything you put intially in S3 in any bucket by default never be public, you have to go and set it after upload.


Alternative to AWS-S3 :-
  
  
S3 VS EBS VS EFS :-

S3: This is the Object-based storage. Each file stored in S3 has a own unique identifier or key, for access through web requests from 
      anywhere from internet or cloufrount. Also S3 support the Static Website Hosting.
      S3 is not at all the traditional file system. 
      Scalability: very high
      Latency: Low
      Its best to use for:-
      storing media content for distribution
      static website
      Archiving
 EBS: Elastic Block Syatem
      This is the Block-Storage 
      In very simple term, EBS is the drive of your EC2 instance. Every EC2 instance must have a EBS attached to it.
      Thing of this as a C or D drive of your vitual EC2 machine.
      Its fastest among all but its scalability is very dificult and required manully attachment/dettachment of EBS (Volume)
      Think of it like a local linux server with EBS as its SSD drive. If load goes up you have to stop server, add more SSD and restrat it.
      Scalability: Poor
      Latency: Lowest
      Its best for 
        Database: sql, nosql etc.
 EFS: Elastic File Syatem
      At a moment just remeber that EFS is same as EBS, But EFS can be attch to multiple EC2 instace and sevices.
      Its biggest advantage over EBS is it is auto scalable whel load fluculated ove attached EC2's.
      Scalability: b/w both
      Latency: b/w both
      
      
  Alternative of S3 ??????????????
  
  
S3 + API Getway for Upload :-
  For this you follow
    First you call a api which will verify that you have the permission for S3 upload or not. This API is coonected with any 
    lenbda authorizatuon function to determine the rights.
    This API then return the secure upload link to you if auth is successful
    Now you use this link to do the upload.
    
    Step 1
      So first create a Role which will be attached to Lembda. This role must have two policy
        step 1.1 basic lemda execution policy [choose predefined one]
        step 1.2 create one policy as such that it has pemsission to write on S3.
   Step 2
      Creste an lembda function with this role, in this lembda function handle shoul look like this-
        IMP parts- 
          UPLOAD_BUCKET_NAME: the name of bucket, should be passed as env var
          filename: the filename of uploading file
          params: ??????
          createPresignedPost: this function will generate secure url for uploading the video

          const handler = (event, context, callback) => {
                const bucket = process.env.UPLOAD_BUCKET_NAME;
                const filename = decodeURI(event.queryStringParameters.filename);
                const directory = crypto.randomBytes(20).toString('hex');
                const key = directory + '/' + filename;

                const params = { 'Bucket': bucket,
                  'Fields': {'key': key},
                  'Conditions': [{'acl': 'private'}]
                }

                s3.createPresignedPost(params, function callbackToPost(error, data) {
                  if (error) {
                     callback(null, generateResponse(400, error));
                  } else {
                    callback(null, generateResponse(200, data));
                  }
                });        
        });
        
        Now at client side use this url for uloading the vidro data.
        
        Step 3:
          Now create an api in API getway, which will expose this lembda function. It better if you also put an another lembda fuctin to check 
          user authorization. This authorizer lembda will detrmine is user is logged in and has authority to execute the lembda created in step 2/
          this authorizer do not detrmine authority to upload the file to s3 for the logged in user.
       Step 4:
           Create an API in api getway, atttach authorizer lembda of step 3 and link generater lembda of step 2 and you are done.
           
           
       
        
   ------------------------------------------
   
   Object VS block storage :-
      To understand the behavior of S3 you musr first focus on its way of working, To understnd the doffrence lets say i wants to sve a 
      100 MB file in storage.
      How diffrent type of storage will behave ? 
      1. Block based -: 
            HEre the file will be splited in small block of 1MB (just assumed) each. then this small puices are saved in memory blocks
            later when you need this 100mb file again, the storage system will retrive those 100 block, arrange them in proper order.
            recreate the 100 MB file again and return it.
            advantage - 
              1. Block storage devices provide low latency IO, so they are suitable for use by Relationshih-databases and OS.
              2. This is best if you wants to do an blob level operation eg. here you can easily append a single line to the end of a log file
             disadvatages :-
              1. if rented from any clod provider (like EC2's EBS) you will have to pay for what size you allocated.
              2. strongly tied to db and server infrastructure strongly, increasunf size of block storage may need a complete infrastrucure overhaul.
              3. You can only access block storage through a running server

        2. Object Based :-
            Here the file is not at splited, instead a file is saved as it is whithout any modification. once stoered it will return a key 
            that will be a identifier of this data or file.
            Now when you need it again you simply use this key to query and you get whole of your file back.
            In simple word - In shoping mall before you enter you deposit your beg to gatekeeper, he gives you a token. later you use this token 
              to get the bag back. The beg is an Data or file (doesnt matted the size or content of bag), the tokn is a key.
            Advangate :-
            1. The content and size of the file doesnt matter, you simply place them in storage, and get the key for later retrival.
                 It could store json, xml, image, video, js, html, mouse(ok not a mouse) almost any kind and size of file/data
            2. Its infinitly expandable, if you need to scale up/down the storage space then add/remove some storage devices and do not need to make any
               changes in rest of the infrastructure.
            3. with the key and proper HTTP api you can get anu data from storage anywhere.
            4. lot of meta data can also be saved along with file/data.
            5. A cost structure that means you only pay for what you use

            
            disadvantage :-
            1. You can’t use object storage services to back a traditional database, due to the high latency of such services
            2. Operating systems can’t easily mount an object store like a normal disk
            3. Object storage doesn’t allow you to alter just a piece of a data blob, you must read and write an entire object at once. 
                This has some performance implications. eg. to add one log line in a log file you have to delete, edit and put entire file again.
      
        
        
--------------------------------        
  
    Bucket name VS key :-
    Bucket name is assosiated with bucket and lie in universal S3 namespace.
    The 'key' belongs to the file names in that bucket.
    EG:
    https://indiaMapImages.s3.eu-west1.amazonaws.com/pune
    Here 'indiaMapImages' is the bucket name and 'pune' is the key.
    The key has to be unique in the bucket.
    
    YOu cannot have two file with same key in a bucket.
    Though you xcan make folder in bucket to use the same key name.
    EG:
    https://indiaMapImages.s3.eu-west1.amazonaws.com/pune
    and 
    https://indiaMapImages.s3.eu-west1.amazonaws.com/distric/pune
    This is two diffrent file reprset by two diffrent key 'pune' and 'distric/pune'
    
    
    --------------------------
    
    
    
    S3 Bucket and CORS:- To get any objevt from S3 we generally use GetObject web-service, now if you call it direcl=tly from 
       browser URL or from Postman, it work fine but if we call it via any other origin eg: localhost.com or www.mufsite.com, 
       or by fetch(..) function of JS, then you may gey the CORS error.
       TO resolve this just add the origin To the bucket it lies in.
       aws > s3 > 'your bucket' > permission > CORS.
       
       Note: The CORS config is set over the Bucket not on the Object
       
       How: google it.
       
       
       
 ---------------------------------------
 S3 and CloudFront(CDN) :-
  When you use the apt 'GET <s3-object-url>' to download any S3 resource, it usues the typiccal internet to load the file to client side.
  Now suppose you have bkt in usa and downloading the file in india, it will take hell of the time. 
  To minimize this time the solution is use CloudFront for the bucket.
  
  Cloud front in itself a large aws service, this serve fast delivery of many S3 servoce like S3, EC2, Rout53 etc
  For now we will see How S3 and CloudFront (CFT) interact.
  
  First lets make a bkt in farthest region possible 'mufa-cdf-enbled-bucket'.
  also say there is many folder in this bucket and you just wants a specific folder ('myDistItems') to be over CFT.
  so create couple of folder including 'myDistItems'. also create 'mufWebsite' and 'mufPriv'
  There is no single setting you have to do here for CFT, it all will be done on AWS > CFT page itself.  
  
  lets create the distribution-
  Steps:-
    Simply go to AWS > Cloud Front > Create > web
    Fill the form, lets see mos imp things to fill for S3:-
    
    Origin Domain Name: 
      Select from the existing value, this will be related to bkt that we have create ealier
      choose option 'mufa-cdf-enbled-bucket.s3.amazonaws.com'. 
      So for S3, this is simply a   <Bucket Name>.s3.amazonaws.com
    Origin Path: 
      This is will be the folder inside that bucket, all the content inside the chossed folder will be accessible via 
      this CFT-distribution. anything outside this folder will not be accessible via this 'CFT-distribution'
      lets type here 'myDistItems'
      
    The above two param plays the vital Role in mapping of Object and CFL paths.
    you have bkt as
      mufa-cdf-enbled-bucket
        fruits
          apple, oranges, mangos [hafua, badam]
        veg
          potato, ginger, onion [big, small] 

    Say you create two diffrent distribution by choosing those
      dist for fruits:-
        Origin Domain Name = mufa-cdf-enbled-bucket.s3.amazonaws.com
        Origin Path = fruits
        ...so..
        The generate CDN url =>  blablabla.cloudfront.net
        By this url you can access - 
            blablabla.cloudfront.net/apple
            blablabla.cloudfront.net/oranges
            blablabla.cloudfront.net/mangos/hafua
         but not
            blablabla.cloudfront.net/fruits/apple  --> That is because the fruots itself is a root folder
            blablabla.cloudfront.net/potato
            blablabla.cloudfront.net/ginger
            blablabla.cloudfront.net/onion/big
            
        
      dist for veg:-
        Origin Domain Name = mufa-cdf-enbled-bucket.s3.amazonaws.com
        Origin Path = veg
        ...so..
        The generate CDN url =>  claclacla.cloudfront.net
        By this url you can access - 
            claclacla.cloudfront.net/potato
            claclacla.cloudfront.net/ginger
            claclacla.cloudfront.net/onion/big
         but not
            claclacla.cloudfront.net/veg/potato
            claclacla.cloudfront.net/apple
            claclacla.cloudfront.net/oranges
            claclacla.cloudfront.net/mangos/hafua  
      
    Restrict Bucket Access: 
      YEs! if you wants to restrict the accessibilitry only and only via CDN, not via direct S3 url
      ?????????????????????????
      
    Origin ID: unique id of this origin, I do not know why they call it a IT. better call it description or alias
      Enter a description for the origin. This value lets you distinguish multiple origins in the same distribution from one another. 
      ??????????????????????????
      
    Origin Access Identity: 
      This is just like(but not same) as IAM user, which is created to access the bucket. 
      Here you can either new identity or use the existing one. [its list is at AWS > CFT > security > Origin Access Identity]
      
    Grant Read Permissions on Bucket: 
      The identity you chossed in option 'Origin Access Identity', must have permission to perform 'GetObject' api on bucket.
      so BP of that bucket to allow the GetObject operation over tye bkt 'mufa-cdf-enbled-bucket'.
      
      If above two option is saetup properly then you will be able to observe two thing after distribution deployed.
      1. at page  AWS > CFT > security > Origin Access Identity  you can see the new identity created with id 'uidOF_OriginAccessIdentity'.
      2. in BP of 'mufa-cdf-enbled-bucket' there is a statement added automatically-
      ...
          "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity <uidOF_OriginAccessIdentity>"
            },
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::mufa-cdf-enbled-bucket/*"          
      ...
    
    Viewer Protocol Policy: 
      you can restric user either to use HTTP or to use HTTPS. Also you can configure to redirect any HTTP 
      to HTTPS. but such redirection will need a SSL certificate to. if you wants it then upload the SSL certifiate in option 'SSL Certificate' at tab 'Distribution Settings'
      Allowed HTTP Methods: which HTTP methodes are allowed, yoyu can allow 'GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE' methodes
      the confusion is, CFT mainly do viewing operatiojn so  How PUT, DELETE, POST works??????????????????????????
    
    Object Caching: 
        Maximum TTL: Once the objevt is cached at Edge-location, it may be possiblr that the file later changed at origin.
          To keepthe edgre location in synch it is must that the edge locations keep itself in synch to that origin.
          The max TTL time is the time that determine that how long the file will live at Edge-location.
          Once the TTL priode passed out the Edge-location communicate wuth origin and synch itself again.
          So any chages at origin will again be refected in edge-locations.
          
          So if you files in s3 buucket/folder is changing very rarly thaen keep the max TTL higher.
          but if it chages a lot then keep it small.expire.
  
   Restrict Viewer Access (Use Signed URLs or Signed Cookies): 
    yes! if you wants to use only personalized signed url to access the files.

-------  

What happen over CFT When
  1. You delete a file from S3 : 
        If you delete a file from S3 and this file may be alaredy over multiple edge location, then its deletion will not be immediatly
        reflected over all ELs, instead the CACHE of this file will remain over the all ELs till its TTL expire. and rhis cached file
        will be served to user.
        Once TTL expire the CFT make a sych request to origin (S3 bkt 'mufa-cdf-enbled-bucket' in our case)
        and look fot this object/file, once he is informed that its deleted then it will reflect it over all trhe ELs. 
        So deletion of file may not reflect immedietly but after the TTL expiration.

  2. Add new file in Bucket:-
       When uploaded upload it, at thattime the file will not be at ELs. becoz S3 upload do not care about CFT. 
       lets say the file key is 'graps' so when you do PUtObject operation on graps.txt, the fi;le will be uploaded obly on S3 bucket. thats it
       Now when any user make a request to CFT (not S3 bucket) for this object by calling 'blablabla.cloudfront.net/graps' 
       The file will be first cached from S3 to the nearest Edge location and then it will be served to the called.
       So in nutshell if new file is added the file be reflected as soon as somone call it via CFT url (no S3 url 'blablabla.s3.amazonaws.com/fruits/graps')
  
  3. When you cvhange the file:
        This work same as the delete one, it will be reflected after the TTL passed out.
        
  How S3-bucket and CFT synch :- 
      When i say su=ynch, it does not mns that all S3 object will be come into the CFT,
      CFT only keep the cache of the file/object that are at least being quried at once, if it never being quried then it will 
      neither it on ELs, nor synch it ever.
      Once the file is quried by any user then only its key is added on CFT and late those only keys are synched.
      When i say synch here, it means that synchronization between 
        CACHED file/object with the origin(Se). '
       not 
        file/object with the origin(Se). '
      
-------

 Q: Does bkt must have all BPAF as off for CloudFront to work: 
    No, its value doesnt matter here.
   
 Q: Doest bucket must be public ?
    No, not neccesarily
 
 Q: Does OBject also has to have a public access
    No, not neccesarily
 
------

Invalidate Cache on CFT:-
  As you know that after the TTL passes out the cache of file/object will get to be synched with origin, but you can also forcefully
  synch it with ortigin. use invalidate option for this.
  
  


 CloudFront VS Transfer-Acceleration : Cloud front is for CDN, its main goal is to make downloading of file asap. But TA is used for
                                        making upload asap.
 
 
       
 --------------------
 
 What is max/object file size S3 upport :will get
 5 TB
       
--------------------
  S3 Signed URLs:-
    Signed url provied you an pre-authenticated url wich is generated via creds of some authorized user. this url can be used used for 2 of operation on S3
    1. GET: read/download object
    2. PUT: upload/update the Object.
    for creating signed url you will need below ingredient-
      Access Id, Secret key, Bucket name, Object name, operation type(getObject, putObject), expiration time
    When you may need this:-
      - You wants to give someone a url to download/upload the object without any creds. 
        the reciver of this url will be able to download/upload the object anywhere without any login.
      - by s3.upload you get the object/file in binary formate (like response: {data: Array[0-100, 101-1000, ...]}) and you need to convert this binary data in strean or file
        by signed url mechanism you can get url to download this file directly.
    EG:-
            AWS.config.update({ credentials: new AWS.Credentials({ accessKeyId: ..., secretAccessKey: .... }) });
            var s3 = new AWS.S3({apiVersion: '2006-03-01'});

            var params = {
              Bucket: document.getElementById("pBucketName").value,
              Key: document.getElementById("objKey").value,
              Expires: 60
            };
            var url = s3.getSignedUrl('getObject', params);
    Acceess and signedUrl:-
      To create the signed url you do not need any permission. but this url will work only if the cred used in the creation process has the proped permission to do operation.
      otherwise user will get the Access denied on that url.

---------------------------------------------
  S3 Signed URLs or Signed Cookies :-
    ????????????????????????????????????????/
  S3 partition:
    ?????????????//
  How to upload a very large file of < 1TB
     
-------------------------

TODO:-
  S3 Select
  VPC S3 endpoint
  Object Lifecycle set up
  S3 Multipart
  S3 AWS JS SDK
  
  



