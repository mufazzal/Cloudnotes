In very Simple word S3 (Simple Storage Service) is STATIC STORAGE. 
A bucket that is best place to put static HTML, css, video, image, static JS  etc.

Things that should be stored in S3 :-
  Any kind of flat file like image, .html. video etc
Wich should not:-
  Any dynamic/processable/database/OS file
  
S3 is Object based storage so you can store file but not OS or database. ?????????
Also data store in form of <key> <value> pair in somewhere ??????? 
 
Bucket:-
  In very simple word, This is a kind of UNIQUE folder in the ENTIRE S3 UNIVERSE
  So when you create it, its name must be quique (not relative to the buckets in your S3 account but in an entire S3 unviverse)
  So you can not take name like test, test123 etc as they may have taken centuries ago. 

Region of S3:-
  S3 is region independent, when you open S3 in aws console you will see region selected as 'Global' and other rigion remian unselectable.
  It means that S3 service remain same in entire aws system.
  If "Test" bucket in created in region "Asia" then "Test" cant be created again by anyone in any region ever.
Region of bucket:
  Though S3 is region independent, but the bucket created in S3 by you will must lie in a selected region 
  When you create a bucket you have to select the region, and so your bucket will lie in that region. 
  VVIMP: it not at all means that content of this bucket wont be avialabler in other region.
  like you create a bucket 'indiaMapImages' in mumbai. 
  but the url 'https:// indiaMapImages.s3.eu-west1.amazonaws.com/pune' will be accessible in anywhere in the world.
  Though the time taken to load the content in newyork will be greater than in mumbai. 
S3 and AZ :- 
  Bucket in region specific, you do not have to choose the AZ at all when you create bucket.
  Depending on your storage class, the content of the bucket will be stored in either 3 or 2 or 2 AZ at atime.
  So if some data lost at some AZ or any AZ is down, you still be able to access the S3 buckets.

S3 has thhe flate file structure. thoug it apear to be have a directiris inside the bucket but s3 actully store it in flt system.
EG:-
  http://s3.mufbucket......com/folder1/folder/2/folder3/image.jpg  
  It does not mean that there is 3 inner folder. It actully mean that a object with key 'folder1/folder/2/folder3/image.jpg'
  For easier viewing experience AWS console also do the logical nesting based on '/' in object key.
  
How To create Bucket :-
  1. aws > S3 > Create Bucket
  2. Fill Name: Mut be unique in all S3 universe 
  3. Select Region: You should select nearest one (VVIMP: Though you select region here but still the name should ne unique to entire S3 universe )
  4. Couple of more thing (will be covered later)


S3 and Data Consistency :-
  on DEC 2020 AWS madr theagic happen. now PUT, DELETE, overwrite PUT(update), 
  all those operation are strongly consistent for GET and LIST operation.
  After a successful write of a 
    new object, or 
    an overwrite or 
    delete of an existing object, 
  any subsequent GET or LIST request immediately receives the latest version of the object.
----------------------------
  



S3 Storage Classes:-
  Storage classes are applied to a Indivisual Object not on Bucket
  1. S3 - Standard : general-purpose storage of frequently accessed data
      Low Latancy - High Throughput -  >= 3 AZ  -
      Storage Fee:  0 till 50TB
      Retrival Fee: NA
      - Best for any general data that need frequent access.
  2. S3 Standard_IA (infrequently Access) : long-lived, but less frequently accessed data
      Low Latancy - High Throughput -  >= 3 AZ  -  all same as standard.
      Storage Fee:  Yes but lower then S3-Standard
      Retrival Fee: Yes
      S3 Standard-IA costs less than S3 Standard in terms of storage price, while still providing the same high durability, throughput, and low latency of S3 Standard.
      But S3 Standard-IA have cost for data retrieval.
      Minimum retention priode and objecte size is 30 days and 128kb. mns even you store a 5kb object for 10 days, u will be charged with 128kb for 30 days. 
      - Best for storing data, that need to access less frequently like at a time of year end or some festival
  3. S3 Standard_IA_one_zone (infrequently Access) : long-lived, but less frequently accessed data, Ok to being in service outage
      This is same as S3-IA except data lies in only one AZ. So risk if AZ goes down and cost lower
      IMP: if AZ goes down then you may face service outage for S3 bucket, but once AZ back online you again got your data and services back.
  4. S3 Inteligent Tiering: Use AI to move data b\w diffrent tiers without your intervention
      IT has two tier, Frequent access tiering And In-Frequent access tiering : -
            If data is not fetched for last 30 days, it will move in 'In-Frequent access tiering'. but as soon as it accessed
            it moved to 'Frequent access tiering'
      Storage Fee:  Yes but very efficient
      Retrival Fee: No but charged for data movement between tiering.
      Minimum retention priode is 30 days. mns even you store a object for 10 days, u will be charged for 30 d
            
  5. S3 Glacier and S3 Glacier deep Archive: for long-term archive and digital preservation
      Take minute and Hours for retrival, Glcier do not have any real time access.
      
    
      

---------------------
Lifecycle Policy:-
  The life cycle policy is apply to a group of object[all in folder] or on whole bucket.
  In lifecycle policy the user define the set of configuration to move the objects in diffrent storage class.
  This is best if you know the lifecycle of the object.
  The is two rule you can define
  Transition Rule:- Rule to determine when and which storage class the object should move
  Expiration rule: set when to delete the object.

Lifecycle Policy VS Inteligent Tiering
  Inteligent Tiering use the amazon developed algorith to move the object between the classes. In Lifecycle Policy the developer himself provide the set of rule to do this.
  EG:-  in Inteligent Tiering the object moves between standard to stanrard_IA in 30 days. in lifecycle you can do set you own rule of time, date, size etc.

-------------------------
Limits:-  
  Default Max No of bucket you can make in AWS account
    - 100, for increasing limit contact to aws service center.
  No of obect in a bucket
    - Infinite
  MAx size of a object that can be uploaded
    - 5 TB
  MAx tag on object
    - 10
  Max limit for PUT:
    - 5GB
    
--------------------   
Charges:
  For Storage
  For Request
  Data Transfer out od S3 (into s3 is free)
  For Storage Management
  Data Transfer
  Transfer Acceleration
  Cross border replication
  Object Tagging
  
---------------------  
Transfer Acceleration: 
  First if you wants to use it then pay for it.
  If you wnants to accelerate the speed of UPLOADing of file on S3 bucket then use it.
  This feature actully uses the heavyweight network of edge-location and cloudFront(instead of typical internet) 
  to maximize the upload speed of the file.
When to use this -
  You have customers that upload to a centralized bucket from all over the world.
  You transfer gigabytes to terabytes of data on a regular basis across continents.
  You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3.

-----------------
Cross Region replication:-
    First if you wants to use it then pay for it.
    It simply allow the bucket to automatically copy to another bucket in diffrent region. So give you kind of data security

-----------------

URL of the files/object in Bcucket:-
  There is 3 diffrent URL pattern to access the S3 object
  
  Virtual region specific
      https://<Bucket Name>.s3.<region Name>.amazonaws.com/<Object Key>  ....#URL_2
  Virtual Global
      https://<Bucket Name>.s3.amazonaws.com/<Object Key>  ....#URL_2
  Path
      https://s3-<region Name>.amazonaws.com/<Bucket Name>/<Object Key>  ....#URL_3
        This style is going to be deprecated.


---------------------

Object Versioning:-
  You can enable the versioning on any object in S3. By this you can keep all the older vesion of that S3 object.
  The versioning is enabled over the object not bucket.
  So 
  Bucket-Name + Object-key + Version gives you the unique s3 object in S3.

-------------------------

Metadata:- S3 also give you the way to store data bout data. EG content-type, content-lengh etc.
Tag:- You can also attach the tag on the Object. 

--------------------------

Object Taging:-
  You cann attach deattach the Tag on Objevt. with the help of tagging you can do below things 
    1. Fine grain access control
    2. Fine grained Lifcycle managment
    3. Cloud watch and cloudtrain metrics filtering

-------------------------
How to craete S3 Bucket and and upload file in it by aws portal:-
  Such Spoonfeeding! really

Once a Bucket is created, These are the operation we can do (Over the bucket not indivisual file)
  1. Enable Versioning
  2. Enable static website hosting
  3. Transfer Acceleration
  4. Create event to emit in case of any opration occure on bucket
  5. Set/Unset permission of IAM user/Group
  6. Set Bucket Policy Json
  7. Set Lifecycle (like moving to glacier after certain time or prvius verion archiving)
  8. Replication to another/same region
  ...etc
  [detail will come later]
  
  
Once a file uploaded on Bucket possible operation on File (not Bucket)
  1. Changing storage class
  2. Changing permission
  ...etc
  
-------------  
  
 Static website in S3:- 
  1. Create a bucket [mufazzalbio], mark it public access, place index.html in it
  2. myWebSiteBucket > properties > Static ebsite hosting --> Enable it and select error and index html page
  3. Though bucket is public but content in it is not so now add bucket policy like this to make all the content in the bucket public
      myWebSiteBucket > Permission > Bucket policy
      
        { "Version": "2012-10-17",
          "Statement": [{
              "Effect": "Allow",
              "Principal": "*",
              "Action": [ "s3:GetObject" ],
              "Resource": [ "arn:aws:s3:::BUCKET_NAME/*"]
            }]
        }      
        
  The url for website be :-
  http://mufazzalbio.s3-website.ap-south-1.amazonaws.com
  
  page/image/file created in folder of bucket will be accessible via same relative path e.g.
    http://mufazzalbio.s3-website.ap-south-1.amazonaws.com/photo/germanyPic.png

#VVIMP: Anything you put intially in S3 in any bucket by default never be public, you have to go and set it after upload.


----------------
  
S3 VS EBS VS EFS :-

S3: This is the Object-based storage. Each file stored in S3 has a own unique identifier or key, for access through web requests from 
      anywhere from internet or cloufrount. Also S3 support the Static Website Hosting.
      S3 is not at all the traditional file system. 
      Scalability: very high
      Latency: Low
      Its best to use for:-
      storing media content for distribution
      static website
      Archiving
 EBS: Elastic Block Syatem
      This is the Block-Storage 
      In very simple term, EBS is the drive of your EC2 instance. Every EC2 instance must have a EBS attached to it.
      Thing of this as a C or D drive of your vitual EC2 machine.
      Its fastest among all but its scalability is very dificult and required manully attachment/dettachment of EBS (Volume)
      Think of it like a local linux server with EBS as its SSD drive. If load goes up you have to stop server, add more SSD and restrat it.
      Scalability: Poor
      Latency: Lowest
      Its best for 
        Database: sql, nosql etc.
 EFS: Elastic File Syatem
      At a moment just remeber that EFS is same as EBS, But EFS can be attch to multiple EC2 instace and sevices.
      Its biggest advantage over EBS is it is auto scalable whel load fluculated ove attached EC2's.
      Scalability: b/w both
      Latency: b/w both
      
      
------------------------------------------
   
   Object VS block storage :-
      To understand the behavior of S3 you musr first focus on its way of working, To understnd the doffrence lets say i wants to sve a 
      100 MB file in storage.
      How diffrent type of storage will behave ? 
      1. Block based -: 
            HEre the file will be splited in small block of 1MB (just assumed) each. then this small puices are saved in memory blocks
            later when you need this 100mb file again, the storage system will retrive those 100 block, arrange them in proper order.
            recreate the 100 MB file again and return it.
            advantage - 
              1. Block storage devices provide low latency IO, so they are suitable for use by Relationshih-databases and OS.
              2. This is best if you wants to do an blob level operation eg. here you can easily append a single line to the end of a log file
             disadvatages :-
              1. if rented from any clod provider (like EC2's EBS) you will have to pay for what size you allocated.
              2. strongly tied to db and server infrastructure strongly, increasunf size of block storage may need a complete infrastrucure overhaul.
              3. You can only access block storage through a running server

        2. Object Based :-
            Here the file is not at splited, instead a file is saved as it is whithout any modification. once stoered it will return a key 
            that will be a identifier of this data or file.
            Now when you need it again you simply use this key to query and you get whole of your file back.
            In simple word - In shoping mall before you enter you deposit your beg to gatekeeper, he gives you a token. later you use this token 
              to get the bag back. The beg is an Data or file (doesnt matted the size or content of bag), the tokn is a key.
            Advangate :-
            1. The content and size of the file doesnt matter, you simply place them in storage, and get the key for later retrival.
                 It could store json, xml, image, video, js, html, mouse(ok not a mouse) almost any kind and size of file/data
            2. Its infinitly expandable, if you need to scale up/down the storage space then add/remove some storage devices and do not need to make any
               changes in rest of the infrastructure.
            3. with the key and proper HTTP api you can get anu data from storage anywhere.
            4. lot of meta data can also be saved along with file/data.
            5. A cost structure that means you only pay for what you use

            
            disadvantage :-
            1. You can’t use object storage services to back a traditional database, due to the high latency of such services
            2. Operating systems can’t easily mount an object store like a normal disk
            3. Object storage doesn’t allow you to alter just a piece of a data blob, you must read and write an entire object at once. 
                This has some performance implications. eg. to add one log line in a log file you have to delete, edit and put entire file again.
      
        
        
--------------------------------        
    
    
    S3 Bucket and CORS:- To get any objevt from S3 we generally use GetObject web-service, now if you call it direcl=tly from 
       browser URL or from Postman, it work fine but if we call it via any other origin eg: localhost.com or www.mufsite.com, 
       or by fetch(..) function of JS, then you may gey the CORS error.
       TO resolve this just add the origin To the bucket it lies in.
       aws > s3 > 'your bucket' > permission > CORS.
       
       Note: The CORS config is set over the Bucket not on the Object
       
       How: google it.
       
       
--------------------
  S3 Signed URLs:-
    Signed url provied you an pre-authenticated url wich is generated via creds of some authorized user. this url can be used used for 2 of operation on S3
    1. GET: read/download object
    2. PUT: upload/update the Object.
    for creating signed url you will need below ingredient-
      Access Id, Secret key, Bucket name, Object name, operation type(getObject, putObject), expiration time
    When you may need this:-
      - You wants to give someone a url to download/upload the object without any creds. 
        the reciver of this url will be able to download/upload the object anywhere without any login.
      - by s3.upload you get the object/file in binary formate (like response: {data: Array[0-100, 101-1000, ...]}) and you need to convert this binary data in strean or file
        by signed url mechanism you can get url to download this file directly.
    EG:-
            AWS.config.update({ credentials: new AWS.Credentials({ accessKeyId: ..., secretAccessKey: .... }) });
            var s3 = new AWS.S3({apiVersion: '2006-03-01'});

            var params = {
              Bucket: document.getElementById("pBucketName").value,
              Key: document.getElementById("objKey").value,
              Expires: 60
            };
            var url = s3.getSignedUrl('getObject', params);
    Acceess and signedUrl:-
      To create the signed url you do not need any permission. but this url will work only if the cred used in the creation process has the proped permission to do operation.
      otherwise user will get the Access denied on that url.

---------------------------------------------
  S3 partition:
    ?????????????//
  How to upload a very large file of < 1TB
     
-------------------------

TODO:-
  S3 Select
  VPC S3 endpoint
  Object Lifecycle set up
  S3 Multipart
  S3 AWS JS SDK
  
  



