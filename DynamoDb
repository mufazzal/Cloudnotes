

-------
Authentication and connection the DDB via AWS-SDK :-
  To perfornm any operation over DDB cia SDG first you have to do some basic setup.
  For this first you have to install the AWS-SDK. say if uou are using the nodejs. the install the SDK via 'npm i aws-sdk'
    Now do the authentication with proper IAM user/role for the SDK.
    For diffrent env -
        1. Authenticating  for non-AWS system :-
            for this read 'AWS JS SDK.txt' #AWSSDK_AUTH_BY_KEY_ID

      2. Authenticating for EC2 :-
          There is two way to do the Authentication here
            1. Via AWS config file : same as of 'non-AWS system'. For this read EC2.txt #AUTHEC2_WAY_1 [never recommended]
            2. By assigning a Role which has 'AmazonDynamoDBFullAccess' Policy attached to it. for more read #AUTHEC2_WAY_2

     3. Authenticating for Lembda :-
          ????????????


  Create Table:
    ?????????????????????  
  
  
Querying the DDB :- 
  Once authentiacation is successful the you can query the data via below code:-              
	var docClient = new AWS.DynamoDB.DocumentClient();
	var params = { TableName: "MufUser",  ...... };
	docClient.get(params, function(err, data) { console.log(err, data) });    
	Params here define that what the data you will get afetr the calling get function.
    
 There is three diffrent technique to read the data. for this you have to provide proper values in params
	and in all three way you have to provide TableName (of course)
	Now lets see those 3 :-
  1 : get By the Primary Key (partition or Sort) :-
        Just provide the partition-key 
        var params = { TableName: "MufUser",  Key:{ "userID": "10" } };	
        This will gives you all the attributes stored against the key '34'. This is most efficient and fastest way.
	out :-
			"Item": {
			    "company": "Amdocs",
			    "name": "Mufazzal",
			    "salary": "200",
			    "userID": "10"
			  }	
        This only work with PK not with other attribute
        So below params will give error in 'get' function
     var params = { TableName: "MufUser",  Key:{ "name": "mufa" } };	  
     Basioclly 'key' in params accept only and only promary key.
     
     PK as Sort Key :-
        ??????????????????
	
     This is most widely used way to get the data because it give whole data for that PK. 
     But it has only one disadvantage. You have to provide the exack key. But lets say you wants to get the info about couple of key
     then its useless.
     
  2 : Query:-
      Query is very useful when you are using the sort key with the partition key. Here once you get the data via your partion key, then 
      you can apply some filter comparison over the sort key.
      For now lets say you hace the Partition-key as 'userID'
      Then thwe query will go-
           var paramsQuery = {
              TableName: "MufUser",
              KeyConditionExpression: "userID = :a",
              ExpressionAttributeValues: { ":a": "34" }
            }     
        Out:
          [ { name: 'eee', genger: 'male', userID: '34' } ]         
          
        KeyConditionExpression and Partition-key :- 
          VVVIMP: in the query function param you can use ONLY ONLY and ONLY 'equal' operatior for Partition-key. you cannot ever use
                  any other operator with partition-key.
                  You can use any operator with 'sort-ket' though. How ????????????????
		So all blow statement simply give an error
		...
		KeyConditionExpression: "userID > :a",
		ExpressionAttributeValues: { ":a": "34" }
		...
		OR
		...
		KeyConditionExpression: "userID IN :a",
		ExpressionAttributeValues: { ":a": {SS: [34, 78, 67]} }
		...
     Query and sort key:-
     	This is the real germ of query. its most useful with sort key. we will see it later
	The first thing you must remeber that in the query you can give one and only one partition-key with equal comparator
	So the 'query' function can only play with the sort-key-combination which lies in that partition-key
	in simple:-
		for the database in fig '#DDB_FIG_PK_SORT' you run the 'qyuery' function and provide the 
		partition-key = 'muffy' then you can play only with r1, r2, or r1=3 but not at all with r4, r5. 
		partition-key = 'fattu' then you can not play with r1, r2, or r3 but only with r4, r5. 
	EG:-
		var paramsQueryWithSort = {
		    TableName: "UserOrder",
		    KeyConditionExpression: "userID = :a",
		    ExpressionAttributeValues: { ":a": "muffy" }
		}
		docClient.query(paramsQueryWithSort, function(err, data) {console.log(err, data)});		
     	The above function will return r1, r2 and r3.
     	Now make a little change:-
		...
	    	KeyConditionExpression: "userID = :a and OrderID = :b",
	    	ExpressionAttributeValues: { ":a": "muffy", ":b": "mob1" }
		...
	This will give only r1
	
		...
		KeyConditionExpression: "userID = :a and begins_with(OrderID, :t)",
	    	ExpressionAttributeValues: { ":a": "muffy", ":t": "mob", }
     		...
	this will give r1 and r2 but nor r3
	
		...
		KeyConditionExpression: "userID = :a and begins_with(OrderID, :t)",
		ExpressionAttributeValues: { ":a": "muffy", ":t": "mob", }
		...
	this will give r3 only.	

	...
	KeyConditionExpression: "userID = :a and begins_with(OrderID, :t)",
	ExpressionAttributeValues: { ":a": "fattu", ":t": "mob", }
	...
	this will give r4 and r5
	
	The KeyConditionExpression accept only the partition-key or sort-key. no one else so below will throw error
	...
	KeyConditionExpression: "userID = :a and price = :b",
	ExpressionAttributeValues: { ":a": "fattu", ":b": "100", }
	...	

	Q: can we design the query in such a way that we can get r1 and r5
	A: Impossible! because r1 and r5 have diffrent partition-key and 'query' methode support only single value of partition-key
	
	Q: So partition-key support only '=', but what operation sort-key do support. Does it support all
	A: NO! it support only following
		sortKeyName = :sortkeyval - true if the sort key value is equal to :sortkeyval.
		sortKeyName < :sortkeyval - true if the sort key value is less than :sortkeyval.
		sortKeyName <= :sortkeyval - true if the sort key value is less than or equal to :sortkeyval.
		sortKeyName > :sortkeyval - true if the sort key value is greater than :sortkeyval.
		sortKeyName >= :sortkeyval - true if the sort key value is greater than or equal to :sortkeyval.
		sortKeyName BETWEEN :sortkeyval1 AND :sortkeyval2 - true if the sort key value is greater than or equal to :sortkeyval1, and less than or equal to :sortkeyval2.
		begins_with ( sortKeyName, :sortkeyval ) - true if the sort key value begins with a particular operand. (You cannot use this function with a sort key that is of type Number.) Note that the function name begins_with is case-sensitive.
	sort-key do not support 'IN()', =%***%, contains(*) etc.
	
		
     Common between 'get' and 'query' both
       - You can also tell in the query that only some specific attribute you wants to read via 'ProjectionExpression'
        	eg: ProjectionExpression: 'name, company, cid'
      
     Filter(NOT QUERY) by attribute :-
		you can furthure filter the result based on the attribute also. for this you must use 'FilterExpression'
		How ???????????????????????
		VVIMP: A filter expression cannot contain partition key or sort key attributes. 
			You need to specify those attributes in the 'KeyConditionExpression', not the 'FilterExpression'
			In 'query' and 'get' both function 'KeyConditionExpression' in required and you have to provide proper condition
			for the the key. but 'FilterExpression' is optional. it just do tyhe after operation of furthur filtering
			So
			A filter expression is applied after a Query finishes, but before the results are returned. 
			Therefore, a Query consumes the same amount of read capacity, regardless of whether a filter expression is present.
	
	
      
          
  3. Scan:
  	To read data this way is exterely resource sensitive. it should be used in very rare case.
	Unlike query it do not need any primary key to run. 
	it run over all the data items in the table. and provide the conditioned filtering on basd of any kind of attribute.
	Though you can provide the 'FilterExpression' However, the filter is applied only after the entire table has been scanned.
	will see it later
	EG:-
			var paramsScan = {
			    TableName: "UserOrder",
			    FilterExpression: 'contains(device,:gen)',
			    ExpressionAttributeValues: {
				":gen": "e"
			    }
			}
			docClient.scan(paramsScan, function(err, data) {console.log(err, data)});	
	This will produce all the data cell which has in its attribute 'device' a letter 'e'. the device is a simple attr not a key.
	in scan the primary key are useless.
	Out:-
		[ {... device: 'samsung'},
		  {... device: 'oneplus'},
		  {... device: 'dell'},
		]
	you can get r1, r2, r3, r4, r5, .... the entire data in below scan
			var paramsScan = { TableName: "UserOrder", }	  
	  
	 Wronglty written scan will produce large data which cant be shipped in an api response. for that the DDB 
	 automatically add the pagination feature in result. in the result if you get the 'LastEvaluatedKey' it means there is more data left
	 to read. 
	 
   Add/Edit data item :-
	For this use 'put' function
	See the example below
		var paramsPut = {
		    TableName: "MufUser",
		    Item: {
			userID: "20",
			name: 'abbas',
			city: "Pune"
		    }
		}
		docClient.put(paramsPut, function(err, data) {console.log(err, data)});	
          
         The main thing here is again param, you must must provide 'tableName' and 'Item' key 
	 In the 'Item' you have to neccesarily provide the partition-key. otherwise there will be an error.
	 
	 put wuth PArtition Key + Sort Key:-
	 	This is almost 99% same as above, exept you also have to provide the Sort-Key 
		if youu do not provide it then it will give an Error.
		EG:
			var paramsPutUOTable = {
			    TableName: "UserOrder",
			    Item: {
				userID: "fattu",
				OrderID: "fattuEaring1",
				metal: 'gold',
				city: "Pune",
				price: 900
			    }
			}
			docClient.put(paramsPutUOTable, function(err, data) {console.log(err, data)});		
		
		You do not have to provide the keys in any special param, just it has to be in the 'Item' above 'userID' and 'OrderID'
		are partition and Sort keys. fail to provide any single of them cause Error.
	
	
	Q: What will happen if the primary key (partition key OR partition key + Srt key) provided in 'put' is already exist
	A: It will be updated with new data.    
          
	Q: Can you insert a data cell in the DDB without Partition-key (along woth sort-key if applicable)?
	A: NO. you have to provide each one if table is design to have them. 
  	
	Quering the data via Attribute :- 
		Either by 'query' or 'get' function you can query the data by partition key OR partition key + Srt key
		but not with any other attributee.
		But if you still wants to make the query possible with any non-partition, non-sort attribute, you have to use 
		GSI (Global Secondry Index) or LSI(Local Secondry Index).
		

-------------------------------------------------------

	PRovision CApicity:-
	   This is used to configure hom much dat can be write or read per second. This value should be chosse on the basis of
	   how much user uses the DDB table at a given time and what is the pattern of usage.
	   You can set it at the time of table creation or later when created already.
	   For new table , its at first form only
	   for exisying table  AWS > DDB > table > 'your table' > capacity.
	   Some fundamental:- 
		   In DDB you generally write data in the cell and the data containg of couple of Attribute with keys.
		   Now first thing you have to figure out is to answer this 4 question
			   A. How much the size of your each data item that you are going to read and write.
			   B. How many read operation you need to do in a second
			   C. How many wtrite operation you do in a second
			   D. You need Eventually-consistent read or Strongly-consistent read 
		   This is solely depend on your app load.
		   
	   There is two way to set them
	   1. Pre assigned (with auto scalling if you wants)		   
					 Now when you were create the table in AWS console, you can see a calculator to calculate the provision there.
					 fill all four value there and you will see 3 numbers in output
						1. The read capacity unit you need 
						2. The write capacity unit you need
						3. The cost you are going to bear for this.
							- So if size your data is large you have to provision higher capascity and have to bear higher cost.
							- If you have lot of data read operation then give higher capacity to read and lesser to write and vise-versa
							- Strongly consistent read will bear the higher cost.
					Now what will happen if suddenly load increase for any read or write operation, For this you set the 
					Auto-Scalling for read:-
						Simply set if to allot minimum   <MNMNMN> unit 
						for read capacity anf if utilization goes above   <PPPP> percentage 
						then add  one more unit and keep it going till the allotment reach to       <MXMXMX> unit
						After that let the thing crash or performance dipped.

					Auto-Scalling for write:-
						Simply set if to allot minimum   <MNMNMN> unit 
						for write capacity anf if utilization goes above   <PPPP> percentage 
						then add  one more unit and keep it going till the allotment reach to       <MXMXMX> unit
						After that let the thing crash or performance dipped.

	   2. On demand
				This is new and the best one. just enbale it and you do not need to set anything . AWS will take care of all things.
			
			
		PRovision	VS On demand:
			PRovision is best if you know the traffic pattern and load on the DB because you can  proviosn capacity and pay less.
			On demand if you do not know the traffic pattern and load on the DB and also your app is new in market so you belive low traffic.
 
			
	
	PRovision CApicity and GSI :-
		When you creat a GSI, it as same as creating a new table. so at that time you will also be asked to set the provision-capasity of 
		GSI table. all the concept same here also except GSI table do not support 'on demand' provisioning.
		So more GSI will cost you more.

	
	What if your throughput goes exceptionalyy high and surpass even the autoscalling max:-
	In that case you will get the 'ProvisionedThroughputExceededException'
	Now if tou are using any AWS-SDK then SDK (if js, java or any) will take care of this and do the retries via 'Exponential Backoff Algorithm'
	SDK handle all this internally so no worry for you.
	But even if retry fail for 1 minute then its a dead end. you will get the error and you have to increase the provisioned capacity.
	
	
----------------------

	DDB Accelerator [DAX] :-
		DAX in in-memory cache for DDB. 
		DAX provide you way to let the frenetly accessed data to remain in cache.
		For this you 
			set up the DAX, 					[HOW ????????????????????????????]
			Point all your API to DAX instead of DDB table		[HOW ????????????????????????????]
		then AWS DDS system take care
		if your API reach to DAX and try to get some data but DAX do not find it then
		DAX make a getItem call to DDB table get it
		Store it in DAX cache
		and return it to api response
		NExt time if some api ask for same data then it will just return from the pre-stored cache.
	   Advantages :-
	   	remove many read overload from DDB and so you may need less read-capacity on DDB table
		Make read fater
	   Limits:-
	   	Not major improvement if data is changing frequently
			If data change in origin DDB table it still may remain unupdated in cache   
			so its only suitable for eventually consistent read. not suitable for Strongly-consistent read
		
		
-------------
	DDB transactoion:-
		Transcation is aset of query which should be all-or-nothing-run.
		This will run all the query or none at all.
		This is useful for operation like credit from A and debit to B.


----------
	DDB and Data Consistency:-
		The DDB have two type of data consistency and 
			1. Eventual Consistent Reads
					Data PUT may take some time to reflect so read may return a minute or two older data.
			2. Strongly Consistent Reads
					Data put will reflect immediatly after the PUT, so data read will always be the latest till the point in time.
					
		it not set at time of creation of DB instead it is depend on how the request is made for read.
			If you want any read operation to be 'Strongly Consistent Reads' then pass the parameter 'ConsistentRead' in GetItem, Query, and Scan.
------------

	DDB and Region:-
		DDB is region dependent.
		if you have a table called People in the us-east-2 Region and another table named People in the us-west-2 Region, these are considered two entirely separate tables

	DDB and AZ:-
		Data automatically replicated across multiple Availability Zones in an AWS Region
	
	DDB and Storage:-
		All of your data is stored on solid-state disks (SSDs)

----------------
	
	DDB and Charges:-
		- Capacity [on demand pr provisoned]
		- DAX
		- GSI
		- Global table
		- Stream
		- Point if time recovery
--------------------

	DDB Stream:-
		This is not enabled by default and its chargable.
		The stream is the record of all the changes (add/edit/delete) in the DDB for the last 24 hour.
		This will help you to build the functionality aroud it.
	
-------------------	
		
	Back up:-
		Manual Back up:-
			You can create a Back up of your DB any time and restore it later whenever you wants. This is very useful for archiving the data
		Point if time recovery:-
			You can enable this feture if you wants to back up the data at any point of time till last 35 days.
			For this the DDB keep the incremental back of last 35 days so that ant accidently write or delete can be restored.				
		
-------------------		
	
	DDB Global Table:-
		For this 'DDB Stream' must be enabled.
		Any DDB table by region specific and lie only in that region. but if you wants to replicate it in other rergion then you have to use the global table
		But for this first you have to enable the 'Stream'
		Once enabled then to set the Global table 
		simply go to table > Global table Add Region
		Now selecc the region and then you can see the same table in that region.
		This table will stay in synch with the orignal one.
		
	
------------
	ElastiCache:-
		This provide same service as DAX, but DAX is specifically for DDB. and Elasticache for mny other.
		for DAX one should always use DAX.
		Set up:-
		?????????????????????????????
		
--------------------

	TTL Attribute:-
		One of the issue with very large database is too keep deleting useless data. For this DDB provide a excellent feature.
		You ca set any particular attribute in the Table as 'TTL Attriute' 
		The data cell will automatically deleted when the time set in that attribute reach
		For this :-
			AWS > DDB > Table > 'your table' > Action > Manage TTL
			Set the attribute that you prefer should consist the value of TTL timestamp.
			lets say you set the attribute 'ExpirationTime'
			Done
		Now entry made by below 'put' request will be deleted after the time reached to '4234234344'	
			var paramsPutExpirationTime = {
			    TableName: "MufUser",
			    Item: {
				userID: "34",
				name: 'ttttt',
				city: "yyyyy",
				ExpirationTime: '4234234344'
			    }
			}			

-------

For AWS-JS-SDK :-
	any dynamoDB function can be conver in promise simply by calling the promise() function.
	the function
			docClient.get(params, function(err, data) {console.log(err, data)});
	is same as below
			docClient.get(params)
			.promise()
			.then(function(data, err) {console.log(err, data)});


--------------------------

	DDB API:-
		Just like SDK where you call the function, there is also exposed API of DDB which you can call.
		??????????????????????????/

		
	
